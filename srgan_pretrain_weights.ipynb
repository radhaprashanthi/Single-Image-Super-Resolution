{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "srgan_pretrain_weights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs6jU7z_V_VY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import Sampler\n",
        "from torchvision.transforms import (Compose, RandomCrop, ToTensor,\n",
        "                                    ToPILImage, CenterCrop, Resize)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import math\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import (\n",
        "    Compose, RandomCrop, ToTensor,\n",
        "    ToPILImage, CenterCrop, Resize,\n",
        "    RandomHorizontalFlip, RandomVerticalFlip)\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import os\n",
        "from torch.cuda.amp import GradScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzU3TVEuWDGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B19uAerQxsgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7edfc51-57f5-45f8-b9b9-a65249af12d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__-Mzg2nX2Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_location = lambda storage, loc: storage if cuda == '' else None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2sb2Dx-yn1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52e0d0a5-725a-4dbb-9968-b019b5c90bb3"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs7HgoXq9QwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36W9QvZoBQP",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/JQ9JL2t/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yz7UUQh9Zuc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/Tcmfjjn/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXOA-fs4r9TF",
        "colab_type": "text"
      },
      "source": [
        "## Fetching the pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtlGESA7os82",
        "colab_type": "code",
        "outputId": "7b55fe23-292d-451c-921e-668428e89f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "! wget https://raw.githubusercontent.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution/master/models.py"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-16 02:36:29--  https://raw.githubusercontent.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution/master/models.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15130 (15K) [text/plain]\n",
            "Saving to: ‘models.py.4’\n",
            "\n",
            "\rmodels.py.4           0%[                    ]       0  --.-KB/s               \rmodels.py.4         100%[===================>]  14.78K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-06-16 02:36:29 (1.18 MB/s) - ‘models.py.4’ saved [15130/15130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1TskSXlsDLu",
        "colab_type": "text"
      },
      "source": [
        "## Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4eEi1Zsq3p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, upscale_factor=4, image_channels=3,\n",
        "                 residual_block_channels=64,\n",
        "                 num_residual_blocks=5):\n",
        "        super().__init__()\n",
        "        self.upscale_factor=upscale_factor\n",
        "        self.image_channels = image_channels\n",
        "        self.residual_block_channels = residual_block_channels\n",
        "        self.num_residual_blocks = num_residual_blocks\n",
        "\n",
        "        # k9n64s1\n",
        "        initial_block_param = {\n",
        "            \"in_channels\": image_channels,\n",
        "            \"kernel_size\": 9,\n",
        "            \"out_channels\": residual_block_channels,\n",
        "            \"stride\": 1,\n",
        "            \"padding\": 9 // 2\n",
        "        }\n",
        "        self.initial_block = nn.Sequential(\n",
        "            nn.Conv2d(**initial_block_param),\n",
        "            nn.PReLU()       \n",
        "        )\n",
        "\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(channels=residual_block_channels) \n",
        "            for _ in range(num_residual_blocks)]\n",
        "        )\n",
        "\n",
        "        self.skip_block = SkipBlock(channels=residual_block_channels)\n",
        "\n",
        "        # two trained sub-pixel convolution layers\n",
        "        num_spcn_blocks = int(math.log(upscale_factor, 2))\n",
        "        self.spcn_blocks = nn.Sequential(\n",
        "            *[SPCNBlock(in_channels=residual_block_channels,\n",
        "                       upscale_factor=2) \n",
        "            for _ in range(num_spcn_blocks)]\n",
        "        )\n",
        "\n",
        "        #  k9n3s1\n",
        "        final_block_param = {\n",
        "            \"in_channels\": residual_block_channels,\n",
        "            \"kernel_size\": 9,\n",
        "            \"out_channels\": image_channels,\n",
        "            \"stride\": 1,\n",
        "            \"padding\": 9 // 2\n",
        "        }\n",
        "        self.final_block = nn.Conv2d(**final_block_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        initial_out = self.initial_block(x)\n",
        "        B_residual_out = self.residual_blocks(initial_out)\n",
        "        skip_out = self.skip_block(B_residual_out,\n",
        "                                   initial_out)\n",
        "        spcn_out = self.spcn_blocks(skip_out)\n",
        "        pixels = self.final_block(spcn_out)\n",
        "        # tanh to squish => [-1, 1]\n",
        "        # add 1          => [0, 2]\n",
        "        # divide by 2    => [0, 1] \n",
        "        # out = (torch.tanh(pixels) + 1) / 2\n",
        "        out = torch.tanh(pixels)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    At the core of our very deep generator network G, which\n",
        "    is illustrated in Figure 4 are B residual blocks with identical\n",
        "    layout. Inspired by Johnson et al. [33] we employ the block\n",
        "    layout proposed by Gross and Wilber [24]. Specifically, we\n",
        "    use two convolutional layers with small 3×3 kernels and 64\n",
        "    feature maps followed by batch-normalization layers [32]\n",
        "    and ParametricReLU [28] as the activation function.\n",
        "\n",
        "    k3n64s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 64,\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, channels=64, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        padding = 3 // 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        # Element-wise sum\n",
        "        return residual + x\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    k3n64s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 64,\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels=64, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        padding = 3 // 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
        "\n",
        "    def forward(self, x, img):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "\n",
        "        # Element-wise sum\n",
        "        return residual + img\n",
        "\n",
        "class SPCNBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    SPCN - sub-pixel convolutional neural network\n",
        "    We increase the resolution of the input image with two trained\n",
        "    sub-pixel convolution layers as proposed by Shi et al. [48].\n",
        "\n",
        "    https://arxiv.org/pdf/1609.05158.pdf\n",
        "\n",
        "    k3n256s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 256, (64 * (2 ** 2))\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, \n",
        "                upscale_factor=2,\n",
        "                kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                            out_channels=in_channels * (upscale_factor ** 2),\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=kernel_size // 2)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=upscale_factor)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axRNfQqDsYGO",
        "colab_type": "text"
      },
      "source": [
        "## Assigning the pretrained weights to the Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjlJC-nSa088",
        "colab_type": "code",
        "outputId": "36002b2d-d3cc-4f58-da4b-a1986958d227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "checkpoint = torch.load(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/sr/model_checkpoint/pre_trained_wandb/checkpoint_srresnet.pth.tar\",\n",
        "    map_location=torch.device('cpu')\n",
        ")"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGvN_6WqXhtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_residual_blocks = len(checkpoint[\"model\"].residual_blocks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpt20gzbga_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model = checkpoint[\"model\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61roCQ5YqQtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator(num_residual_blocks=num_residual_blocks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMOMmJHrlzPk",
        "colab_type": "code",
        "outputId": "4dfd1c3b-5bf0-4e2f-dfc1-5752ac38c0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_num_parameters(model):\n",
        "    return sum(param.numel() for param in model.parameters())\n",
        "\n",
        "get_num_parameters(generator_model) == get_num_parameters(pretrained_model)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZe_Opa0tML3",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the resuidual blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdVkNrVGrLhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(len(generator_model.residual_blocks)):\n",
        "    from_block = pretrained_model.residual_blocks[idx]\n",
        "    to_block = generator_model.residual_blocks[idx]\n",
        "    \n",
        "    to_block.conv1 = from_block.conv_block1.conv_block[0]\n",
        "    to_block.bn1 = from_block.conv_block1.conv_block[1]\n",
        "    to_block.prelu = from_block.conv_block1.conv_block[2]\n",
        "\n",
        "    to_block.conv2 = from_block.conv_block2.conv_block[0]\n",
        "    to_block.bn2 = from_block.conv_block2.conv_block[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isDVyJMVrwa0",
        "colab_type": "code",
        "outputId": "4e0d7993-3abd-42d3-9233-9fc91bec1a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.equal(\n",
        "    pretrained_model.residual_blocks[0].conv_block1.conv_block[0].weight,\n",
        "    generator_model.residual_blocks[0].conv1.weight\n",
        ")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1GTl9nXta5S",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the initial block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_KzwpejuDv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model.initial_block[0] = pretrained_model.conv_block1.conv_block[0]\n",
        "generator_model.initial_block[1] = pretrained_model.conv_block1.conv_block[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VKfxBTt8T6",
        "colab_type": "code",
        "outputId": "aef62cb8-f928-4586-c53e-51fba865cda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.equal(\n",
        "    generator_model.initial_block[0].weight,\n",
        "    pretrained_model.conv_block1.conv_block[0].weight\n",
        ")"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH7VG3zvto5U",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the final block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6s8hmvGt_2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model.final_block = pretrained_model.conv_block3.conv_block[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxcF9nLZucmy",
        "colab_type": "code",
        "outputId": "367a3527-398d-4b9d-a7fe-8c4a6b438c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.equal(\n",
        "    generator_model.final_block.weight,\n",
        "    pretrained_model.conv_block3.conv_block[0].weight\n",
        ")"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3gpQcabtzs4",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the spcn block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPUq97u2wFUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(len(pretrained_model.subpixel_convolutional_blocks)):\n",
        "    from_block = pretrained_model.subpixel_convolutional_blocks[idx]\n",
        "    to_block = generator_model.spcn_blocks[idx]\n",
        "\n",
        "    to_block.conv = from_block.conv\n",
        "    to_block.pixel_shuffle = from_block.pixel_shuffle\n",
        "    to_block.prelu = from_block.prelu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t81CcjnbvOZQ",
        "colab_type": "code",
        "outputId": "e976cc4c-5473-4fb1-c0f6-82e7ff2e643e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.equal(\n",
        "    generator_model.spcn_blocks[0].conv.weight,\n",
        "    pretrained_model.subpixel_convolutional_blocks[0].conv.weight\n",
        ")"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ubts78It_RB",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the skip block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqzHejXbvrda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model.skip_block.conv1 = pretrained_model.conv_block2.conv_block[0]\n",
        "generator_model.skip_block.bn1 = pretrained_model.conv_block2.conv_block[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQbRGtViv9p7",
        "colab_type": "code",
        "outputId": "68451e33-7615-45ac-9652-662585db3009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.equal(\n",
        "    generator_model.skip_block.conv1.weight,\n",
        "    pretrained_model.conv_block2.conv_block[0].weight\n",
        ")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr21W6hgxhfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_generator_checkpoint(model, filepath, is_amp=False):\n",
        "    \n",
        "    checkpoint = {\n",
        "        'upscale_factor': model.upscale_factor,\n",
        "        'image_channels': model.image_channels,\n",
        "        'residual_block_channels': model.residual_block_channels,\n",
        "        'num_residual_blocks': model.num_residual_blocks,\n",
        "        'state_dict': model.state_dict(),\n",
        "     }\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "\n",
        "def load_generator_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = Generator(checkpoint['upscale_factor'],\n",
        "                      checkpoint['image_channels'],\n",
        "                      checkpoint['residual_block_channels'],\n",
        "                      checkpoint['num_residual_blocks'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ-OFs6MztDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = Path(\"/content/drive/My Drive/Colab Notebooks/DL/final_project/aishu/model_checkpoint\")\n",
        "os.makedirs(PATH, exist_ok=True)\n",
        "save_generator_checkpoint(generator_model, PATH / \"generator_pretrianed_to_new.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWmUDEjuMmE",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF8Fz3CfuPOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    We follow the architectural guidelines summarized by Radford et al. [44] \n",
        "    and use LeakyReLU activation (α = 0.2) and avoid max-pooling throughout\n",
        "    the discriminator network. It contains eight convolutional layers with \n",
        "    an increasing number of 3 × 3 filter kernels, increasing by a factor of \n",
        "    2 from 64 to 512 kernels as in the VGG network [49]. \n",
        "    Strided convolutions are used to reduce the image resolution each time \n",
        "    the number of features is doubled. The resulting 512 feature maps are\n",
        "    followed by two dense layers and a final sigmoid activation\n",
        "    function to obtain a probability for sample classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size=3, image_channels=3, num_middle_blocks = 7):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.image_channels = image_channels\n",
        "        self.num_middle_blocks = num_middle_blocks\n",
        "        # k3n64s1\n",
        "        initial_block_param = {\n",
        "            \"in_channels\": image_channels,\n",
        "            \"kernel_size\": kernel_size,\n",
        "            \"out_channels\": 64,\n",
        "            \"stride\": 1,\n",
        "            \"padding\": kernel_size // 2\n",
        "        }\n",
        "\n",
        "        self.initial_block = nn.Sequential(\n",
        "            nn.Conv2d(**initial_block_param),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        # 7 middle blocks from 64 channels to 512 channnels\n",
        "        in_channels = 64\n",
        "        middle_blocks = []\n",
        "        for i in range(num_middle_blocks):\n",
        "            if i % 2 == 0:\n",
        "                stride = 2\n",
        "                out_channels = in_channels\n",
        "            else:\n",
        "                stride = 1\n",
        "                out_channels = in_channels * 2\n",
        "            middle_blocks.append(\n",
        "                MiddleBlock(in_channels = in_channels, \n",
        "                            out_channels = out_channels, \n",
        "                            kernel_size = kernel_size , \n",
        "                            stride=stride))\n",
        "            in_channels = out_channels\n",
        "        \n",
        "        self.middle_blocks = nn.Sequential(*middle_blocks)\n",
        "        \n",
        "        # final linear layers\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.linear1 = nn.Linear(out_channels * 6 * 6, 1024)\n",
        "        self.leakyRelu = nn.LeakyReLU(0.2)\n",
        "        self.linear2 = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        init_out = self.initial_block(x)\n",
        "        mid_out = self.middle_blocks(init_out)\n",
        "        out = self.adaptive_pool(mid_out)\n",
        "        out = self.linear1(out.view(batch_size, -1))\n",
        "        out = self.leakyRelu(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "    \n",
        "class MiddleBlock(nn.Module):\n",
        "   \n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
        "                            out_channels=out_channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.leakyRelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.bn1(output)\n",
        "        output = self.leakyRelu(output)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBZhfZsvHfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_size=3\n",
        "image_channels=3\n",
        "num_middle_blocks=7\n",
        "\n",
        "discriminator_model = Discriminator(kernel_size=kernel_size, image_channels=image_channels, num_middle_blocks = num_middle_blocks).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTjdxJJX9epV",
        "colab_type": "text"
      },
      "source": [
        "## Assisgning the pretrained weights to the discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgAZ5-VzyP02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "67469c93-ce3f-4ad4-fb80-b29c8937fc24"
      },
      "source": [
        "checkpoint = torch.load(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/sr/model_checkpoint/pre_trained_wandb/checkpoint_srgan.pth.tar\",\n",
        "    map_location=torch.device('cpu')\n",
        ")"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdfVXLZHy-mI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model = checkpoint[\"discriminator\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCfiqN4q1wTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c8ee1ea-34d6-4963-ce58-94baea0d9c4d"
      },
      "source": [
        "get_num_parameters(discriminator_model) == get_num_parameters(pretrained_model)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1VjgNZ9py-",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the middle blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xVxDOQx3C2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(len(discriminator_model.middle_blocks)):\n",
        "    from_block = pretrained_model.conv_blocks[idx + 1]\n",
        "    to_block = discriminator_model.middle_blocks[idx]\n",
        "    to_block.conv1 = from_block.conv_block[0]\n",
        "    to_block.bn1 = from_block.conv_block[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJfwPSzm4u5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a75dd7c2-842c-467e-bb08-6168c50872e5"
      },
      "source": [
        "torch.equal(\n",
        "    pretrained_model.conv_blocks[1].conv_block[0].weight,\n",
        "    discriminator_model.middle_blocks[0].conv1.weight\n",
        ")"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CyCkkn99vMs",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the initial block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPiX9Z_47AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_model.initial_block[0] = pretrained_model.conv_blocks[0].conv_block[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ynseW87SRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f864b93e-a1e8-4640-85c4-bb6d6c323c3f"
      },
      "source": [
        "torch.equal(\n",
        "    discriminator_model.initial_block[0].weight,\n",
        "    pretrained_model.conv_blocks[0].conv_block[0].weight\n",
        ")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzbSaz9g91H1",
        "colab_type": "text"
      },
      "source": [
        "#### Copying the weights of the final linear layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2YKIGUH7lmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_model.linear1 = pretrained_model.fc1\n",
        "discriminator_model.linear2 = pretrained_model.fc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByZ3rY5k8pMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a5edc73-6b8f-4e70-ff99-98ebc268fd85"
      },
      "source": [
        "torch.equal(\n",
        "    discriminator_model.linear1.weight,\n",
        "    pretrained_model.fc1.weight\n",
        ")"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW4t5_Oc-YYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_discriminator_checkpoint(model, filepath):\n",
        "    \n",
        "    checkpoint = {'kernel_size' : model.kernel_size,\n",
        "                  'image_channels': model.image_channels,\n",
        "                  'num_middle_blocks': model.num_middle_blocks,\n",
        "                  'state_dict': model.state_dict()}\n",
        "\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "\n",
        "\n",
        "def load_discriminator_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = Generator(checkpoint['kernel_size'],\n",
        "                      checkpoint['image_channels'],\n",
        "                      checkpoint['num_middle_blocks'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sl7r-7Y8vxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = Path(\"/content/drive/My Drive/Colab Notebooks/DL/final_project/aishu/model_checkpoint\")\n",
        "os.makedirs(PATH, exist_ok=True)\n",
        "save_discriminator_checkpoint(discriminator_model, PATH / \"discriminator_pretrianed_to_new.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}