{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_srgan_v0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe9020fb065a4da781b1f036562aa236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a41845583221472b90a95eed72485f29",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6f9bc2a66be4522b822228943b4c5ac",
              "IPY_MODEL_949e56fc8ce54d8b80f303460cd15796"
            ]
          }
        },
        "a41845583221472b90a95eed72485f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6f9bc2a66be4522b822228943b4c5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a46290545304643be56db68599a88f7",
            "_dom_classes": [],
            "description": " 99%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 149,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91f8ac4259c545faa351c385669c08a5"
          }
        },
        "949e56fc8ce54d8b80f303460cd15796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d50bd2b221db44d2a78b2bfff4d31503",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 149/150 [1:32:19&lt;00:33, 33.68s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dd65109f1b3433e8f73317e6b782563"
          }
        },
        "9a46290545304643be56db68599a88f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91f8ac4259c545faa351c385669c08a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d50bd2b221db44d2a78b2bfff4d31503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dd65109f1b3433e8f73317e6b782563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhaprashanthi/Single-Image-Super-Resolution/blob/master/01_srgan_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Y-LmvlEj80",
        "colab_type": "text"
      },
      "source": [
        "SRGAN Trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtpW4Q55uEc",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocv40SzZ45dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import Sampler\n",
        "from torchvision.transforms import (Compose, RandomCrop, ToTensor,\n",
        "                                    ToPILImage, CenterCrop, Resize,\n",
        "                                    Normalize, Lambda)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import math\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import (\n",
        "    Compose, RandomCrop, ToTensor,\n",
        "    ToPILImage, CenterCrop, Resize,\n",
        "    RandomHorizontalFlip, RandomVerticalFlip)\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import os\n",
        "from torch.cuda.amp import GradScaler"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mms51g8cfwOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "848cfd1d-dd69-450b-a5a9-706fb4188725"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJomI9gItG07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "# rm -rf apex\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dUBg88FbRuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !sh setup.sh"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36W9QvZoBQP",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/JQ9JL2t/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER0iXuxGABNU",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/Tcmfjjn/image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszi1BWmE3SO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8890ab2-172f-4af3-b6de-1ad55e1e3680"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCrqMJcHBJ-u",
        "colab_type": "text"
      },
      "source": [
        "## Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKUBeamG0ReY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, upscale_factor=4, image_channels=3,\n",
        "                 residual_block_channels=64,\n",
        "                 num_residual_blocks=5):\n",
        "        super().__init__()\n",
        "        self.upscale_factor=upscale_factor\n",
        "        self.image_channels = image_channels\n",
        "        self.residual_block_channels = residual_block_channels\n",
        "        self.num_residual_blocks = num_residual_blocks\n",
        "\n",
        "        # k9n64s1\n",
        "        initial_block_param = {\n",
        "            \"in_channels\": image_channels,\n",
        "            \"kernel_size\": 9,\n",
        "            \"out_channels\": residual_block_channels,\n",
        "            \"stride\": 1,\n",
        "            \"padding\": 9 // 2\n",
        "        }\n",
        "        self.initial_block = nn.Sequential(\n",
        "            nn.Conv2d(**initial_block_param),\n",
        "            nn.PReLU()       \n",
        "        )\n",
        "\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(channels=residual_block_channels) \n",
        "            for _ in range(num_residual_blocks)]\n",
        "        )\n",
        "\n",
        "        self.skip_block = SkipBlock(channels=residual_block_channels)\n",
        "\n",
        "        # two trained sub-pixel convolution layers\n",
        "        num_spcn_blocks = int(math.log(upscale_factor, 2))\n",
        "        self.spcn_blocks = nn.Sequential(\n",
        "            *[SPCNBlock(in_channels=residual_block_channels,\n",
        "                       upscale_factor=2) \n",
        "            for _ in range(num_spcn_blocks)]\n",
        "        )\n",
        "\n",
        "        #  k9n3s1\n",
        "        final_block_param = {\n",
        "            \"in_channels\": residual_block_channels,\n",
        "            \"kernel_size\": 9,\n",
        "            \"out_channels\": image_channels,\n",
        "            \"stride\": 1,\n",
        "            \"padding\": 9 // 2\n",
        "        }\n",
        "        self.final_block = nn.Conv2d(**final_block_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        initial_out = self.initial_block(x)\n",
        "        B_residual_out = self.residual_blocks(initial_out)\n",
        "        skip_out = self.skip_block(B_residual_out,\n",
        "                                   initial_out)\n",
        "        spcn_out = self.spcn_blocks(skip_out)\n",
        "        pixels = self.final_block(spcn_out)\n",
        "        # tanh to squish => [-1, 1]\n",
        "        out = torch.tanh(pixels)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    At the core of our very deep generator network G, which\n",
        "    is illustrated in Figure 4 are B residual blocks with identical\n",
        "    layout. Inspired by Johnson et al. [33] we employ the block\n",
        "    layout proposed by Gross and Wilber [24]. Specifically, we\n",
        "    use two convolutional layers with small 3×3 kernels and 64\n",
        "    feature maps followed by batch-normalization layers [32]\n",
        "    and ParametricReLU [28] as the activation function.\n",
        "\n",
        "    k3n64s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 64,\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, channels=64, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        padding = 3 // 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        # Element-wise sum\n",
        "        return residual + x\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    k3n64s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 64,\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels=64, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        padding = 3 // 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
        "                            out_channels=channels,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=padding,\n",
        "                            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
        "\n",
        "    def forward(self, x, img):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "\n",
        "        # Element-wise sum\n",
        "        return residual + img\n",
        "\n",
        "class SPCNBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    SPCN - sub-pixel convolutional neural network\n",
        "    We increase the resolution of the input image with two trained\n",
        "    sub-pixel convolution layers as proposed by Shi et al. [48].\n",
        "\n",
        "    https://arxiv.org/pdf/1609.05158.pdf\n",
        "\n",
        "    k3n256s1 => \n",
        "        kernel_size = 3,\n",
        "        channels = 256, (64 * (2 ** 2))\n",
        "        stride = 1\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, \n",
        "                upscale_factor=2,\n",
        "                kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                            out_channels=in_channels * (upscale_factor ** 2),\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=kernel_size // 2)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=upscale_factor)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo_x0s5BhJfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upscale_factor=4\n",
        "image_channels=3\n",
        "residual_block_channels=64\n",
        "num_residual_blocks=16\n",
        "\n",
        "generator = Generator(upscale_factor=upscale_factor,\n",
        "                      image_channels=image_channels,\n",
        "                      residual_block_channels=residual_block_channels,\n",
        "                      num_residual_blocks=num_residual_blocks).to(device)\n",
        "#generator"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_YtYJ_ApViG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b6bc092-3141-4432-db23-f5bdf5bc9a6c"
      },
      "source": [
        "sum(param.numel() for param in generator.parameters())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1549462"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj3JfMJsBQ9e",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-Processing and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Vxc_6Emm1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "mean =  torch.FloatTensor([0.485, 0.456, 0.406])\n",
        "std =  torch.FloatTensor([0.229, 0.224, 0.225])\n",
        "\n",
        "img_mean = mean.view([3, 1, 1])\n",
        "img_std = std.view([3, 1, 1])\n",
        "batch_mean = mean.view([1, 3, 1, 1])\n",
        "batch_std = std.view([1, 3, 1, 1])\n",
        "\n",
        "# normalize = Normalize(mean=mean,\n",
        "#                       std=std)\n",
        "\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size),\n",
        "        RandomHorizontalFlip(),\n",
        "        RandomVerticalFlip(),\n",
        "        ToTensor(),\n",
        "        Lambda(imagenet_normalise),\n",
        "    ])\n",
        "\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
        "        ToTensor(),\n",
        "        # normalize\n",
        "    ])\n",
        "\n",
        "def valid_hr_transform():\n",
        "    return Compose([\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "def valid_lr_transform(crop_size):\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(crop_size, interpolation=Image.BICUBIC),\n",
        "        ToTensor(),\n",
        "        Lambda(imagenet_normalise),\n",
        "    ])\n",
        "\n",
        "output_to_pil_image = Compose(\n",
        "    [\n",
        "     # add 1          => [0, 2]\n",
        "     # divide by 2    => [0, 1]\n",
        "     Lambda(lambda img: (img + 1.0) / 2.0),\n",
        "     ToPILImage()\n",
        "    ]\n",
        ")\n",
        "\n",
        "def imagenet_normalise(img):\n",
        "    input_device = img.device\n",
        "    if img.ndimension() == 3:\n",
        "        img = (img - img_mean.to(input_device)) / img_std.to(input_device)\n",
        "    elif img.ndimension() == 4:\n",
        "        img = (img - batch_mean.to(input_device)) / batch_std.to(input_device)\n",
        "    return img\n",
        "\n",
        "output_to_imagenet = Compose(\n",
        "    [\n",
        "     # add 1          => [0, 2]\n",
        "     # divide by 2    => [0, 1]\n",
        "     Lambda(lambda img: (img + 1.0) / 2.0),\n",
        "     Lambda(imagenet_normalise),\n",
        "    ]\n",
        ")\n",
        "\n",
        "class SRImageDataset(Dataset):\n",
        "    def __init__(self, dataset_dir, \n",
        "                 crop_size=100, \n",
        "                 upscale_factor=4, is_valid=False):\n",
        "        super().__init__()\n",
        "        glob_path = str(Path(dataset_dir) / \"*[.jpg, .png]\")\n",
        "        self.crop_size = max_crop_size(crop_size, upscale_factor)\n",
        "        self.upscale_factor = upscale_factor\n",
        "        \n",
        "        self.image_filenames = glob(glob_path)\n",
        "        self.is_valid = is_valid\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pil_hr_image = Image.open(self.image_filenames[index])\n",
        "        if self.is_valid:\n",
        "            hr_image = valid_hr_transform()(pil_hr_image)\n",
        "            crop_size = (int(pil_hr_image.height / 4),\n",
        "                         int(pil_hr_image.width / 4))\n",
        "            lr_image = valid_lr_transform(crop_size)(hr_image)\n",
        "        else:\n",
        "            hr_image = train_hr_transform(self.crop_size)(pil_hr_image)\n",
        "            lr_image = train_lr_transform(self.crop_size, self.upscale_factor)(hr_image)\n",
        "        \n",
        "        return lr_image, hr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnLcQ-PFp1Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SRImageDataset(\n",
        "    dataset_dir=\"/content/drive/My Drive/Colab Notebooks/sr/training_data\",\n",
        "    crop_size=32\n",
        ")\n",
        "\n",
        "valid_dataset = SRImageDataset(\n",
        "    dataset_dir=\"/content/drive/My Drive/Colab Notebooks/sr/DIV2K_valid_HR\",\n",
        "    crop_size=500\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czquRnq4vj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d532b4d5-2291-4855-ae10-62ba2e36be89"
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=200,\n",
        "    num_workers=4\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset=valid_dataset, batch_size=10,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "len(train_dataloader), len(valid_dataloader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hiPIKU_Bbrh",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBdNGTK0-I4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_segment(start_lr, end_lr, iterations):\n",
        "    i = np.arange(iterations)\n",
        "    c_i = 1 + np.cos(i*np.pi/iterations)\n",
        "    return end_lr + (start_lr - end_lr)/2 *c_i\n",
        "\n",
        "def get_cosine_triangular_lr(max_lr, iterations, div_start=5, div_end=5):\n",
        "    min_start, min_end = max_lr/div_start, max_lr/div_end\n",
        "    iter1 = int(0.3*iterations)\n",
        "    iter2 = iterations - iter1\n",
        "    segs = [cosine_segment(min_start, max_lr, iter1), cosine_segment(max_lr, min_end, iter2)]\n",
        "    return np.concatenate(segs)\n",
        "\n",
        "def update_optimizer(optimizer, lr):\n",
        "    for i, param_group in enumerate(optimizer.param_groups):\n",
        "        param_group[\"lr\"] = lr"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsWwOqAJC4QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/drive/My Drive/ML/sr/model_checkpoint/srgan_checkpoints/aishu/srgan_v1.pth\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XexD8DVZ-LW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_metrics(model, valid_dl):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "    y_hat = []\n",
        "    ys = []\n",
        "    for lr, hr in valid_dl:\n",
        "        lr, hr = lr.to(device), hr.to(device)\n",
        "        batch = hr.shape[0]\n",
        "        out = model(lr) #.cuda()\n",
        "        loss = F.mse_loss(out, hr) #.cuda()\n",
        "        sum_loss += batch*(loss.item())\n",
        "        total += batch\n",
        "\n",
        "    return sum_loss/total\n",
        "\n",
        "def train_model(model, train_dl, valid_dl, optimizer,\n",
        "                max_lr=0.05, epochs=100,\n",
        "                save_freq=5, filepath=filepath,\n",
        "                is_amp=False):\n",
        "    iterations = epochs * len(train_dl)\n",
        "    pbar = tqdm(total=iterations)\n",
        "    idx = 0\n",
        "    best_val_r2 = 0\n",
        "\n",
        "    # scaler = GradScaler()\n",
        "    \n",
        "\n",
        "    for t in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total = 0\n",
        "        for lr, hr in train_dl:\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "            batch_size = hr.size(0)\n",
        "            sr = model(lr)\n",
        "            sr = output_to_imagenet(sr)\n",
        "            loss = F.mse_loss(sr, hr)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss.backward()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                # scaled_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # optimizer.step()\n",
        "            # scaler.step(optimizer)\n",
        "            \n",
        "            total_loss += loss.item() * batch_size\n",
        "            total += batch_size\n",
        "            idx +=1\n",
        "            pbar.update()\n",
        "            \n",
        "            # Updates the scale for next iteration\n",
        "            # scaler.update()\n",
        "            del lr, hr, sr\n",
        "        \n",
        "        # val_loss = val_metrics(model, valid_dl)\n",
        "\n",
        "        if t % save_freq == 0:\n",
        "            save_generator_checkpoint(model, filepath, is_amp)\n",
        "        print(f\"Training Loss: {total_loss/total}\")\n",
        "        # print(f\"Training Loss: {total_loss/total}, Valid Loss: {val_loss}\")\n",
        "            \n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OOXLoC6AeP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_generator_checkpoint(model, filepath, is_amp=False):\n",
        "    \n",
        "    checkpoint = {'upscale_factor': model.upscale_factor,\n",
        "                  'image_channels': model.image_channels,\n",
        "                  'residual_block_channels': model.residual_block_channels,\n",
        "                  'num_residual_blocks': model.num_residual_blocks,\n",
        "                  'state_dict': model.state_dict(),\n",
        "                  }\n",
        "    if is_amp:\n",
        "        checkpoint['amp'] = amp.state_dict()\n",
        "        checkpoint['optimizer'] = optimizer.state_dict()\n",
        "\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "\n",
        "\n",
        "def load_generator_checkpoint(filepath, is_amp=False):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = Generator(checkpoint['upscale_factor'],\n",
        "                      checkpoint['image_channels'],\n",
        "                      checkpoint['residual_block_channels'],\n",
        "                      checkpoint['num_residual_blocks'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    if is_amp:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        amp.load_state_dict(checkpoint['amp'])\n",
        "    model.to(device)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9muKwsUNXkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = [param for param in generator.parameters() if  param.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.01, weight_decay=1e-5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aPxft8AvZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/drive/My Drive/Colab Notebooks/sr/model_checkpoint/srgan_checkpoints/aishu/srgan_v1.pth\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQpReKcNtFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "fe9020fb065a4da781b1f036562aa236",
            "a41845583221472b90a95eed72485f29",
            "d6f9bc2a66be4522b822228943b4c5ac",
            "949e56fc8ce54d8b80f303460cd15796",
            "9a46290545304643be56db68599a88f7",
            "91f8ac4259c545faa351c385669c08a5",
            "d50bd2b221db44d2a78b2bfff4d31503",
            "5dd65109f1b3433e8f73317e6b782563"
          ]
        },
        "outputId": "e870cfa1-d68c-4c12-e10e-59e23f371674"
      },
      "source": [
        "generator = train_model(generator, train_dataloader,\n",
        "                        valid_dataloader, optimizer,\n",
        "                        epochs=10, save_freq=5,\n",
        "                        filepath=filepath,\n",
        "                        is_amp=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe9020fb065a4da781b1f036562aa236",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 4.832177214261864\n",
            "Training Loss: 5.023463250542221\n",
            "Training Loss: 4.865523781763072\n",
            "Training Loss: 4.998108013671319\n",
            "Training Loss: 4.864860832524233\n",
            "Training Loss: 4.890283511132419\n",
            "Training Loss: 4.97477237049605\n",
            "Training Loss: 4.834294550225181\n",
            "Training Loss: 4.911273645085781\n",
            "Training Loss: 4.900651075592896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAXKyBMoO3G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr, hr = train_dataset[100]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjfgSw_MFsgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.eval()\n",
        "sr = (\n",
        "    generator(\n",
        "        lr.unsqueeze(0).to(device)\n",
        "    ).to(\"cpu\")\n",
        "    .float()\n",
        "    .squeeze(0)\n",
        "    .detach()\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksHq1ccYsh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a859b49-eb8e-4ab8-80f3-0a49922581da"
      },
      "source": [
        "ToPILImage()(lr)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAxklEQVR4nAXBTU/CMBgA4Pbt1vKxjY8NxWpM4ObZhDtnriT8Rv+LZxPPRgyuk22O0m7dW56HbreH/DnZb15Zj5SiOrXH9w9VaVAyXS+y6ld7x03n+VDkcv65CCEbRKl4KE/m+6voKWi0x/wHewvMkUIVfCDKqhyHQ6y71hCCIiD1xXlQ3RnQcubLq4pGXFsbyPsp8cBFAGyizoXpnWvM7r8B+bQKeRJP4mQ2qv6amCVCiJeLgcflHYMoSyVtGUNOkOi6fptNbwcZXj1S2aQ3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=8x8 at 0x7F91B64A3518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRM-2gKjS04r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "c69b0bd3-6286-49b3-8b1d-109beb73842c"
      },
      "source": [
        "ToPILImage()(hr)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F91B64BD470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_5FVChYxxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "bbe3b2d6-d6f0-4c64-806e-32700f1b65af"
      },
      "source": [
        "ToPILImage()(output_to_imagenet(sr))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAALUlEQVR4nGN88tOEgZaAiaamj1owasGoBaMWjFowasGoBaMWjFowasGoBVQEAJxUAlHa6SjXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F91B64BDAC8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249RLdFL2yLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}