{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root_path = Path(\"..\")\n",
    "\n",
    "assert(root_path.exists())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from srresnet.train import *\n",
    "\n",
    "#1396 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bc44b8c00f473ebe88148c75d93a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.083192 (0.073540)\n",
      "1 MSE loss 0.063462 (0.073400)\n",
      "2 MSE loss 0.065697 (0.072782)\n",
      "3 MSE loss 0.049440 (0.072518)\n",
      "4 MSE loss 0.071539 (0.072019)\n",
      "5 MSE loss 0.074862 (0.071277)\n",
      "6 MSE loss 0.079695 (0.071413)\n",
      "7 MSE loss 0.057822 (0.071684)\n",
      "8 MSE loss 0.037993 (0.071228)\n",
      "9 MSE loss 0.070807 (0.070563)\n",
      "10 MSE loss 0.060637 (0.070467)\n",
      "11 MSE loss 0.047513 (0.070480)\n",
      "12 MSE loss 0.066303 (0.070456)\n",
      "13 MSE loss 0.063299 (0.070558)\n",
      "14 MSE loss 0.057235 (0.070687)\n",
      "15 MSE loss 0.058548 (0.070606)\n",
      "16 MSE loss 0.059583 (0.070634)\n",
      "17 MSE loss 0.079805 (0.070578)\n",
      "18 MSE loss 0.064677 (0.070701)\n",
      "19 MSE loss 0.064119 (0.070616)\n",
      "20 MSE loss 0.076316 (0.070482)\n",
      "21 MSE loss 0.058237 (0.070431)\n",
      "22 MSE loss 0.051523 (0.070622)\n",
      "23 MSE loss 0.052256 (0.070481)\n",
      "24 MSE loss 0.053808 (0.070474)\n",
      "25 MSE loss 0.082655 (0.070431)\n",
      "26 MSE loss 0.075196 (0.070489)\n",
      "27 MSE loss 0.085434 (0.070486)\n",
      "28 MSE loss 0.068998 (0.070412)\n",
      "29 MSE loss 0.059251 (0.070496)\n",
      "30 MSE loss 0.066874 (0.070423)\n",
      "31 MSE loss 0.073409 (0.070360)\n",
      "32 MSE loss 0.061932 (0.070367)\n",
      "33 MSE loss 0.044965 (0.070396)\n",
      "34 MSE loss 0.094362 (0.070428)\n",
      "35 MSE loss 0.073847 (0.070418)\n",
      "36 MSE loss 0.051697 (0.070397)\n",
      "37 MSE loss 0.060406 (0.070408)\n",
      "38 MSE loss 0.049114 (0.070427)\n",
      "39 MSE loss 0.078383 (0.070429)\n",
      "40 MSE loss 0.056894 (0.070420)\n",
      "41 MSE loss 0.083167 (0.070444)\n",
      "42 MSE loss 0.072106 (0.070480)\n",
      "43 MSE loss 0.052582 (0.070542)\n",
      "44 MSE loss 0.071562 (0.070476)\n",
      "45 MSE loss 0.066365 (0.070462)\n",
      "46 MSE loss 0.064824 (0.070420)\n",
      "47 MSE loss 0.056669 (0.070473)\n",
      "48 MSE loss 0.063112 (0.070392)\n",
      "49 MSE loss 0.057367 (0.070437)\n",
      "50 MSE loss 0.082039 (0.070392)\n",
      "51 MSE loss 0.062985 (0.070441)\n",
      "52 MSE loss 0.062083 (0.070452)\n",
      "53 MSE loss 0.041771 (0.070429)\n",
      "54 MSE loss 0.040806 (0.070371)\n",
      "55 MSE loss 0.045490 (0.070343)\n",
      "56 MSE loss 0.044281 (0.070225)\n",
      "57 MSE loss 0.073047 (0.070213)\n",
      "58 MSE loss 0.045813 (0.070184)\n",
      "59 MSE loss 0.069651 (0.070194)\n",
      "60 MSE loss 0.080725 (0.070137)\n",
      "61 MSE loss 0.053848 (0.070156)\n",
      "62 MSE loss 0.064614 (0.070181)\n",
      "63 MSE loss 0.058105 (0.070188)\n",
      "64 MSE loss 0.062238 (0.070158)\n",
      "65 MSE loss 0.074840 (0.070153)\n",
      "66 MSE loss 0.061738 (0.070172)\n",
      "67 MSE loss 0.072910 (0.070151)\n",
      "68 MSE loss 0.070621 (0.070126)\n",
      "69 MSE loss 0.072836 (0.070186)\n",
      "70 MSE loss 0.069765 (0.070194)\n",
      "71 MSE loss 0.077206 (0.070219)\n",
      "72 MSE loss 0.049934 (0.070173)\n",
      "73 MSE loss 0.061469 (0.070155)\n",
      "74 MSE loss 0.077606 (0.070186)\n",
      "75 MSE loss 0.044402 (0.070172)\n",
      "76 MSE loss 0.086190 (0.070180)\n",
      "77 MSE loss 0.098354 (0.070156)\n",
      "78 MSE loss 0.055394 (0.070157)\n",
      "79 MSE loss 0.080822 (0.070198)\n",
      "80 MSE loss 0.080192 (0.070192)\n",
      "81 MSE loss 0.093137 (0.070238)\n",
      "82 MSE loss 0.075018 (0.070237)\n",
      "83 MSE loss 0.087134 (0.070229)\n",
      "84 MSE loss 0.065802 (0.070189)\n",
      "85 MSE loss 0.050672 (0.070215)\n",
      "86 MSE loss 0.056538 (0.070230)\n",
      "87 MSE loss 0.066272 (0.070216)\n",
      "88 MSE loss 0.055980 (0.070177)\n",
      "89 MSE loss 0.052754 (0.070167)\n",
      "90 MSE loss 0.068020 (0.070138)\n",
      "91 MSE loss 0.058260 (0.070147)\n",
      "92 MSE loss 0.051061 (0.070157)\n",
      "93 MSE loss 0.040541 (0.070149)\n",
      "94 MSE loss 0.049688 (0.070157)\n",
      "95 MSE loss 0.074082 (0.070110)\n",
      "96 MSE loss 0.065693 (0.070100)\n",
      "97 MSE loss 0.057182 (0.070060)\n",
      "98 MSE loss 0.048846 (0.070072)\n",
      "99 MSE loss 0.055821 (0.070052)\n",
      "100 MSE loss 0.075469 (0.070023)\n",
      "101 MSE loss 0.047529 (0.069990)\n",
      "102 MSE loss 0.051081 (0.069998)\n",
      "103 MSE loss 0.061963 (0.069966)\n",
      "104 MSE loss 0.064891 (0.069983)\n",
      "105 MSE loss 0.063400 (0.069963)\n",
      "106 MSE loss 0.051157 (0.069941)\n",
      "107 MSE loss 0.079685 (0.069966)\n",
      "108 MSE loss 0.037379 (0.069975)\n",
      "109 MSE loss 0.057999 (0.069961)\n",
      "110 MSE loss 0.056433 (0.069944)\n",
      "111 MSE loss 0.061975 (0.069928)\n",
      "112 MSE loss 0.049877 (0.069949)\n",
      "113 MSE loss 0.058724 (0.069935)\n",
      "114 MSE loss 0.052759 (0.069927)\n",
      "115 MSE loss 0.047218 (0.069921)\n",
      "116 MSE loss 0.071214 (0.069894)\n",
      "117 MSE loss 0.073889 (0.069902)\n",
      "118 MSE loss 0.050568 (0.069877)\n",
      "119 MSE loss 0.052938 (0.069892)\n",
      "120 MSE loss 0.062615 (0.069866)\n",
      "121 MSE loss 0.041704 (0.069851)\n",
      "122 MSE loss 0.054412 (0.069839)\n",
      "123 MSE loss 0.052556 (0.069823)\n",
      "124 MSE loss 0.047624 (0.069786)\n",
      "125 MSE loss 0.070925 (0.069775)\n",
      "126 MSE loss 0.054509 (0.069770)\n",
      "127 MSE loss 0.093913 (0.069789)\n",
      "128 MSE loss 0.079316 (0.069795)\n",
      "129 MSE loss 0.052562 (0.069805)\n",
      "130 MSE loss 0.097084 (0.069826)\n",
      "131 MSE loss 0.065608 (0.069837)\n",
      "132 MSE loss 0.089336 (0.069836)\n",
      "133 MSE loss 0.037400 (0.069801)\n",
      "134 MSE loss 0.076110 (0.069806)\n",
      "135 MSE loss 0.058534 (0.069807)\n",
      "136 MSE loss 0.078161 (0.069812)\n",
      "137 MSE loss 0.048618 (0.069827)\n",
      "138 MSE loss 0.051934 (0.069818)\n",
      "139 MSE loss 0.076050 (0.069821)\n",
      "140 MSE loss 0.069498 (0.069800)\n",
      "141 MSE loss 0.042437 (0.069811)\n",
      "142 MSE loss 0.057395 (0.069791)\n",
      "143 MSE loss 0.077000 (0.069753)\n",
      "144 MSE loss 0.072057 (0.069742)\n",
      "145 MSE loss 0.052954 (0.069753)\n",
      "146 MSE loss 0.059123 (0.069729)\n",
      "147 MSE loss 0.068597 (0.069702)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f5efb942a4407ea54507bf48b284a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.070099 (0.078738)\n",
      "1 MSE loss 0.076476 (0.076487)\n",
      "2 MSE loss 0.089645 (0.076609)\n",
      "3 MSE loss 0.047507 (0.074765)\n",
      "4 MSE loss 0.065178 (0.074050)\n",
      "5 MSE loss 0.047060 (0.073443)\n",
      "6 MSE loss 0.082950 (0.072952)\n",
      "7 MSE loss 0.050605 (0.073204)\n",
      "8 MSE loss 0.082135 (0.073485)\n",
      "9 MSE loss 0.047020 (0.073372)\n",
      "10 MSE loss 0.069938 (0.072890)\n",
      "11 MSE loss 0.053300 (0.072734)\n",
      "12 MSE loss 0.050766 (0.072218)\n",
      "13 MSE loss 0.045215 (0.072214)\n",
      "14 MSE loss 0.079746 (0.072331)\n",
      "15 MSE loss 0.075017 (0.072378)\n",
      "16 MSE loss 0.088946 (0.072322)\n",
      "17 MSE loss 0.092942 (0.072222)\n",
      "18 MSE loss 0.055696 (0.072282)\n",
      "19 MSE loss 0.074385 (0.072247)\n",
      "20 MSE loss 0.032975 (0.072363)\n",
      "best loss:  tensor(0.0330, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "21 MSE loss 0.047533 (0.072307)\n",
      "22 MSE loss 0.038373 (0.072199)\n",
      "23 MSE loss 0.093583 (0.072098)\n",
      "24 MSE loss 0.049237 (0.072048)\n",
      "25 MSE loss 0.070276 (0.072086)\n",
      "26 MSE loss 0.088164 (0.071965)\n",
      "27 MSE loss 0.083548 (0.071964)\n",
      "28 MSE loss 0.073442 (0.071925)\n",
      "29 MSE loss 0.065237 (0.071917)\n",
      "30 MSE loss 0.060845 (0.072036)\n",
      "31 MSE loss 0.052209 (0.072089)\n",
      "32 MSE loss 0.071653 (0.072092)\n",
      "33 MSE loss 0.069589 (0.072075)\n",
      "34 MSE loss 0.065758 (0.072071)\n",
      "35 MSE loss 0.067920 (0.072022)\n",
      "36 MSE loss 0.074840 (0.071982)\n",
      "37 MSE loss 0.048288 (0.071972)\n",
      "38 MSE loss 0.081468 (0.071990)\n",
      "39 MSE loss 0.064380 (0.071935)\n",
      "40 MSE loss 0.070784 (0.072034)\n",
      "41 MSE loss 0.097541 (0.072042)\n",
      "42 MSE loss 0.090021 (0.072037)\n",
      "43 MSE loss 0.069538 (0.071968)\n",
      "44 MSE loss 0.045921 (0.071990)\n",
      "45 MSE loss 0.066185 (0.071958)\n",
      "46 MSE loss 0.065053 (0.072075)\n",
      "47 MSE loss 0.053810 (0.072119)\n",
      "48 MSE loss 0.046243 (0.072126)\n",
      "49 MSE loss 0.036056 (0.072010)\n",
      "50 MSE loss 0.086069 (0.071942)\n",
      "51 MSE loss 0.056036 (0.071840)\n",
      "52 MSE loss 0.076602 (0.071817)\n",
      "53 MSE loss 0.058194 (0.071794)\n",
      "54 MSE loss 0.063883 (0.071698)\n",
      "55 MSE loss 0.062543 (0.071788)\n",
      "56 MSE loss 0.058886 (0.071795)\n",
      "57 MSE loss 0.037785 (0.071777)\n",
      "58 MSE loss 0.068782 (0.071765)\n",
      "59 MSE loss 0.065849 (0.071705)\n",
      "60 MSE loss 0.072696 (0.071727)\n",
      "61 MSE loss 0.062981 (0.071798)\n",
      "62 MSE loss 0.092540 (0.071797)\n",
      "63 MSE loss 0.066340 (0.071818)\n",
      "64 MSE loss 0.056350 (0.071814)\n",
      "65 MSE loss 0.068444 (0.071850)\n",
      "66 MSE loss 0.067292 (0.071886)\n",
      "67 MSE loss 0.057772 (0.071909)\n",
      "68 MSE loss 0.055945 (0.071896)\n",
      "69 MSE loss 0.053780 (0.071856)\n",
      "70 MSE loss 0.074770 (0.071790)\n",
      "71 MSE loss 0.071502 (0.071773)\n",
      "72 MSE loss 0.079079 (0.071774)\n",
      "73 MSE loss 0.063707 (0.071775)\n",
      "74 MSE loss 0.066753 (0.071795)\n",
      "75 MSE loss 0.072718 (0.071762)\n",
      "76 MSE loss 0.091851 (0.071706)\n",
      "77 MSE loss 0.088912 (0.071684)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4bf58f2f23430b94dd23a1d66eba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.061490 (0.074329)\n",
      "1 MSE loss 0.065854 (0.075963)\n",
      "2 MSE loss 0.070263 (0.075794)\n",
      "3 MSE loss 0.071709 (0.074983)\n",
      "4 MSE loss 0.066237 (0.074748)\n",
      "5 MSE loss 0.078358 (0.074500)\n",
      "6 MSE loss 0.068591 (0.074784)\n",
      "7 MSE loss 0.082046 (0.074367)\n",
      "8 MSE loss 0.079164 (0.074188)\n",
      "9 MSE loss 0.060313 (0.073633)\n",
      "10 MSE loss 0.060631 (0.073725)\n",
      "11 MSE loss 0.104313 (0.073707)\n",
      "12 MSE loss 0.056559 (0.073574)\n",
      "13 MSE loss 0.046826 (0.073651)\n",
      "14 MSE loss 0.082368 (0.073446)\n",
      "15 MSE loss 0.063255 (0.073393)\n",
      "16 MSE loss 0.033916 (0.073216)\n",
      "best loss:  tensor(0.0339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "17 MSE loss 0.090705 (0.073221)\n",
      "18 MSE loss 0.049708 (0.073513)\n",
      "19 MSE loss 0.061223 (0.073381)\n",
      "20 MSE loss 0.054011 (0.073279)\n",
      "21 MSE loss 0.086637 (0.073175)\n",
      "22 MSE loss 0.048615 (0.072927)\n",
      "23 MSE loss 0.049130 (0.072850)\n",
      "24 MSE loss 0.100261 (0.072947)\n",
      "25 MSE loss 0.058919 (0.073021)\n",
      "26 MSE loss 0.062420 (0.072976)\n",
      "27 MSE loss 0.061739 (0.072940)\n",
      "28 MSE loss 0.072548 (0.072917)\n",
      "29 MSE loss 0.063678 (0.072961)\n",
      "30 MSE loss 0.060806 (0.073026)\n",
      "31 MSE loss 0.069411 (0.073137)\n",
      "32 MSE loss 0.091299 (0.073095)\n",
      "33 MSE loss 0.064034 (0.072988)\n",
      "34 MSE loss 0.059323 (0.072997)\n",
      "35 MSE loss 0.063784 (0.073018)\n",
      "36 MSE loss 0.063092 (0.072992)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dff9e59c33488d9378a21f6bfa1d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.071896 (0.074738)\n",
      "best loss:  tensor(0.0719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "1 MSE loss 0.106558 (0.075765)\n",
      "2 MSE loss 0.066223 (0.075297)\n",
      "best loss:  tensor(0.0662, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "3 MSE loss 0.062865 (0.074458)\n",
      "best loss:  tensor(0.0629, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "4 MSE loss 0.056394 (0.074304)\n",
      "best loss:  tensor(0.0564, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "5 MSE loss 0.045745 (0.074290)\n",
      "best loss:  tensor(0.0457, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "6 MSE loss 0.076670 (0.074278)\n",
      "7 MSE loss 0.081102 (0.074420)\n",
      "8 MSE loss 0.060843 (0.074684)\n",
      "9 MSE loss 0.050462 (0.074465)\n",
      "10 MSE loss 0.075517 (0.074684)\n",
      "11 MSE loss 0.074668 (0.074730)\n",
      "12 MSE loss 0.048231 (0.074722)\n",
      "13 MSE loss 0.067683 (0.074404)\n",
      "14 MSE loss 0.084399 (0.074512)\n",
      "15 MSE loss 0.041176 (0.074530)\n",
      "best loss:  tensor(0.0412, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "16 MSE loss 0.048500 (0.074200)\n",
      "17 MSE loss 0.083898 (0.074058)\n",
      "18 MSE loss 0.052284 (0.073979)\n",
      "19 MSE loss 0.053899 (0.073867)\n",
      "20 MSE loss 0.066835 (0.074044)\n",
      "21 MSE loss 0.081298 (0.073950)\n",
      "22 MSE loss 0.058057 (0.073838)\n",
      "23 MSE loss 0.056215 (0.073845)\n",
      "24 MSE loss 0.089882 (0.073833)\n",
      "25 MSE loss 0.077955 (0.073911)\n",
      "26 MSE loss 0.039567 (0.073799)\n",
      "best loss:  tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "27 MSE loss 0.055401 (0.073854)\n",
      "28 MSE loss 0.096946 (0.073789)\n",
      "29 MSE loss 0.052064 (0.073790)\n",
      "30 MSE loss 0.063814 (0.073689)\n",
      "31 MSE loss 0.069125 (0.073807)\n",
      "32 MSE loss 0.073495 (0.073851)\n",
      "33 MSE loss 0.073733 (0.073795)\n",
      "34 MSE loss 0.063368 (0.073815)\n",
      "35 MSE loss 0.038182 (0.073928)\n",
      "best loss:  tensor(0.0382, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "36 MSE loss 0.065203 (0.073931)\n",
      "37 MSE loss 0.068010 (0.073965)\n",
      "38 MSE loss 0.048992 (0.073957)\n",
      "39 MSE loss 0.069700 (0.073921)\n",
      "40 MSE loss 0.074386 (0.073886)\n",
      "41 MSE loss 0.069664 (0.073834)\n",
      "42 MSE loss 0.063536 (0.073806)\n",
      "43 MSE loss 0.061870 (0.073705)\n",
      "44 MSE loss 0.093381 (0.073620)\n",
      "45 MSE loss 0.042424 (0.073580)\n",
      "46 MSE loss 0.035228 (0.073519)\n",
      "best loss:  tensor(0.0352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "47 MSE loss 0.097102 (0.073587)\n",
      "48 MSE loss 0.073158 (0.073575)\n",
      "49 MSE loss 0.068301 (0.073571)\n",
      "50 MSE loss 0.076028 (0.073542)\n",
      "51 MSE loss 0.063790 (0.073505)\n",
      "52 MSE loss 0.082070 (0.073450)\n",
      "53 MSE loss 0.062109 (0.073449)\n",
      "54 MSE loss 0.076859 (0.073456)\n",
      "55 MSE loss 0.064284 (0.073481)\n",
      "56 MSE loss 0.077446 (0.073500)\n",
      "57 MSE loss 0.048420 (0.073500)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d448430bbd45dcb6cac738c8b2cbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.057673 (0.076099)\n",
      "best loss:  tensor(0.0577, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "1 MSE loss 0.064547 (0.076553)\n",
      "2 MSE loss 0.084779 (0.076709)\n",
      "3 MSE loss 0.075719 (0.077478)\n",
      "4 MSE loss 0.066861 (0.077577)\n",
      "5 MSE loss 0.043462 (0.077983)\n",
      "best loss:  tensor(0.0435, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "6 MSE loss 0.071705 (0.077907)\n",
      "7 MSE loss 0.053504 (0.077411)\n",
      "8 MSE loss 0.073891 (0.077288)\n",
      "9 MSE loss 0.064800 (0.077395)\n",
      "10 MSE loss 0.065179 (0.077071)\n",
      "11 MSE loss 0.074480 (0.076737)\n",
      "12 MSE loss 0.054388 (0.076423)\n",
      "13 MSE loss 0.052185 (0.076337)\n",
      "14 MSE loss 0.087406 (0.076185)\n",
      "15 MSE loss 0.069188 (0.076230)\n",
      "16 MSE loss 0.060382 (0.076033)\n",
      "17 MSE loss 0.041375 (0.075983)\n",
      "best loss:  tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "18 MSE loss 0.090651 (0.076081)\n",
      "19 MSE loss 0.049381 (0.075925)\n",
      "20 MSE loss 0.070132 (0.075815)\n",
      "21 MSE loss 0.056256 (0.075831)\n",
      "22 MSE loss 0.052656 (0.075844)\n",
      "23 MSE loss 0.056126 (0.075784)\n",
      "24 MSE loss 0.102772 (0.075798)\n",
      "25 MSE loss 0.059990 (0.075723)\n",
      "26 MSE loss 0.101199 (0.075773)\n",
      "27 MSE loss 0.065341 (0.075827)\n",
      "28 MSE loss 0.068157 (0.075895)\n",
      "29 MSE loss 0.091618 (0.075971)\n",
      "30 MSE loss 0.077378 (0.075909)\n",
      "31 MSE loss 0.053884 (0.075844)\n",
      "32 MSE loss 0.090125 (0.075847)\n",
      "33 MSE loss 0.078211 (0.076016)\n",
      "34 MSE loss 0.055376 (0.076046)\n",
      "35 MSE loss 0.071706 (0.076084)\n",
      "36 MSE loss 0.106853 (0.076043)\n",
      "37 MSE loss 0.057444 (0.075928)\n",
      "38 MSE loss 0.075173 (0.075972)\n",
      "39 MSE loss 0.081084 (0.075956)\n",
      "40 MSE loss 0.086990 (0.076085)\n",
      "41 MSE loss 0.075787 (0.076147)\n",
      "42 MSE loss 0.079839 (0.076108)\n",
      "43 MSE loss 0.048955 (0.076193)\n",
      "44 MSE loss 0.075458 (0.076116)\n",
      "45 MSE loss 0.086036 (0.076080)\n",
      "46 MSE loss 0.067046 (0.076083)\n",
      "47 MSE loss 0.080207 (0.076050)\n",
      "48 MSE loss 0.072544 (0.076028)\n",
      "49 MSE loss 0.058229 (0.075958)\n",
      "50 MSE loss 0.049612 (0.075891)\n",
      "51 MSE loss 0.060980 (0.075855)\n",
      "52 MSE loss 0.078498 (0.075839)\n",
      "53 MSE loss 0.055475 (0.075797)\n",
      "54 MSE loss 0.054504 (0.075678)\n",
      "55 MSE loss 0.068129 (0.075716)\n",
      "56 MSE loss 0.049265 (0.075711)\n",
      "57 MSE loss 0.059040 (0.075641)\n",
      "58 MSE loss 0.058957 (0.075601)\n",
      "59 MSE loss 0.094108 (0.075594)\n",
      "60 MSE loss 0.065879 (0.075594)\n",
      "61 MSE loss 0.080131 (0.075553)\n",
      "62 MSE loss 0.056941 (0.075572)\n",
      "63 MSE loss 0.092506 (0.075540)\n",
      "64 MSE loss 0.069214 (0.075528)\n",
      "65 MSE loss 0.058878 (0.075484)\n",
      "66 MSE loss 0.105906 (0.075501)\n",
      "67 MSE loss 0.068751 (0.075471)\n",
      "68 MSE loss 0.079393 (0.075479)\n",
      "69 MSE loss 0.076178 (0.075488)\n",
      "70 MSE loss 0.059334 (0.075481)\n",
      "71 MSE loss 0.092006 (0.075485)\n",
      "72 MSE loss 0.043852 (0.075430)\n",
      "73 MSE loss 0.073650 (0.075370)\n",
      "74 MSE loss 0.066179 (0.075310)\n",
      "75 MSE loss 0.047333 (0.075253)\n",
      "76 MSE loss 0.056827 (0.075222)\n",
      "77 MSE loss 0.045537 (0.075205)\n",
      "78 MSE loss 0.058359 (0.075212)\n",
      "79 MSE loss 0.057360 (0.075145)\n",
      "80 MSE loss 0.108068 (0.075165)\n",
      "81 MSE loss 0.068183 (0.075174)\n",
      "82 MSE loss 0.082121 (0.075199)\n",
      "83 MSE loss 0.094117 (0.075207)\n",
      "84 MSE loss 0.067228 (0.075185)\n",
      "85 MSE loss 0.053672 (0.075121)\n",
      "86 MSE loss 0.046373 (0.075068)\n",
      "87 MSE loss 0.051103 (0.075028)\n",
      "88 MSE loss 0.086958 (0.074994)\n",
      "89 MSE loss 0.056044 (0.075003)\n",
      "90 MSE loss 0.049656 (0.075030)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9795ceeea7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/final_project/srresnet/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(root_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                         filepath=filepath)\n\u001b[0m",
      "\u001b[0;32m~/final_project/srresnet/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, device, train_dataloader, filepath, epochs, save_freq)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(root_path) #500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ce4b33ad1b415f9afd6c9864dba82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.089799 (0.091351)\n",
      "1 MSE loss 0.081833 (0.090448)\n",
      "2 MSE loss 0.091590 (0.090272)\n",
      "3 MSE loss 0.091460 (0.089868)\n",
      "4 MSE loss 0.070718 (0.089669)\n",
      "5 MSE loss 0.090204 (0.089442)\n",
      "6 MSE loss 0.074972 (0.089198)\n",
      "7 MSE loss 0.086138 (0.089355)\n",
      "8 MSE loss 0.087284 (0.089203)\n",
      "9 MSE loss 0.072435 (0.089554)\n",
      "10 MSE loss 0.075061 (0.089375)\n",
      "11 MSE loss 0.091500 (0.089402)\n",
      "12 MSE loss 0.075106 (0.089435)\n",
      "13 MSE loss 0.086529 (0.089288)\n",
      "14 MSE loss 0.072812 (0.089313)\n",
      "15 MSE loss 0.081093 (0.089213)\n",
      "16 MSE loss 0.064930 (0.089178)\n",
      "17 MSE loss 0.090254 (0.089154)\n",
      "18 MSE loss 0.087066 (0.089309)\n",
      "19 MSE loss 0.073570 (0.089570)\n",
      "20 MSE loss 0.086193 (0.089836)\n",
      "21 MSE loss 0.085349 (0.089883)\n",
      "22 MSE loss 0.078834 (0.089959)\n",
      "23 MSE loss 0.093439 (0.089809)\n",
      "24 MSE loss 0.090879 (0.089708)\n",
      "25 MSE loss 0.090040 (0.089796)\n",
      "26 MSE loss 0.067911 (0.089795)\n",
      "27 MSE loss 0.062584 (0.089650)\n",
      "28 MSE loss 0.086511 (0.089551)\n",
      "29 MSE loss 0.066915 (0.089496)\n",
      "30 MSE loss 0.084376 (0.089358)\n",
      "31 MSE loss 0.104570 (0.089440)\n",
      "32 MSE loss 0.118909 (0.089404)\n",
      "33 MSE loss 0.111524 (0.089494)\n",
      "34 MSE loss 0.068694 (0.089532)\n",
      "35 MSE loss 0.099728 (0.089515)\n",
      "36 MSE loss 0.099487 (0.089507)\n",
      "37 MSE loss 0.060424 (0.089459)\n",
      "38 MSE loss 0.083747 (0.089485)\n",
      "39 MSE loss 0.095780 (0.089460)\n",
      "40 MSE loss 0.107670 (0.089475)\n",
      "41 MSE loss 0.051825 (0.089429)\n",
      "42 MSE loss 0.124196 (0.089445)\n",
      "43 MSE loss 0.084425 (0.089507)\n",
      "44 MSE loss 0.082489 (0.089407)\n",
      "45 MSE loss 0.074985 (0.089381)\n",
      "46 MSE loss 0.091885 (0.089359)\n",
      "47 MSE loss 0.090352 (0.089404)\n",
      "48 MSE loss 0.089035 (0.089387)\n",
      "49 MSE loss 0.079077 (0.089328)\n",
      "50 MSE loss 0.081944 (0.089366)\n",
      "51 MSE loss 0.094463 (0.089394)\n",
      "52 MSE loss 0.079514 (0.089377)\n",
      "53 MSE loss 0.101689 (0.089305)\n",
      "54 MSE loss 0.071965 (0.089263)\n",
      "55 MSE loss 0.106083 (0.089254)\n",
      "56 MSE loss 0.076161 (0.089230)\n",
      "57 MSE loss 0.068793 (0.089230)\n",
      "58 MSE loss 0.063094 (0.089197)\n",
      "59 MSE loss 0.071954 (0.089192)\n",
      "60 MSE loss 0.065804 (0.089201)\n",
      "61 MSE loss 0.113184 (0.089131)\n",
      "62 MSE loss 0.072955 (0.089072)\n",
      "63 MSE loss 0.054916 (0.089058)\n",
      "64 MSE loss 0.073027 (0.089018)\n",
      "65 MSE loss 0.108351 (0.088988)\n",
      "66 MSE loss 0.103405 (0.088999)\n",
      "67 MSE loss 0.061082 (0.088933)\n",
      "68 MSE loss 0.114466 (0.088905)\n",
      "69 MSE loss 0.087532 (0.088882)\n",
      "70 MSE loss 0.061183 (0.088815)\n",
      "71 MSE loss 0.074836 (0.088845)\n",
      "72 MSE loss 0.094840 (0.088817)\n",
      "73 MSE loss 0.092421 (0.088778)\n",
      "74 MSE loss 0.079024 (0.088769)\n",
      "75 MSE loss 0.082338 (0.088770)\n",
      "76 MSE loss 0.071598 (0.088788)\n",
      "77 MSE loss 0.084958 (0.088748)\n",
      "78 MSE loss 0.092752 (0.088780)\n",
      "79 MSE loss 0.083486 (0.088782)\n",
      "80 MSE loss 0.096031 (0.088707)\n",
      "81 MSE loss 0.095232 (0.088727)\n",
      "82 MSE loss 0.072927 (0.088738)\n",
      "83 MSE loss 0.083165 (0.088763)\n",
      "84 MSE loss 0.073508 (0.088707)\n",
      "85 MSE loss 0.066826 (0.088723)\n",
      "86 MSE loss 0.093169 (0.088758)\n",
      "87 MSE loss 0.102675 (0.088792)\n",
      "88 MSE loss 0.084706 (0.088806)\n",
      "89 MSE loss 0.097447 (0.088769)\n",
      "90 MSE loss 0.091556 (0.088721)\n",
      "91 MSE loss 0.085138 (0.088678)\n",
      "92 MSE loss 0.090815 (0.088699)\n",
      "93 MSE loss 0.072810 (0.088679)\n",
      "94 MSE loss 0.089454 (0.088696)\n",
      "95 MSE loss 0.106093 (0.088693)\n",
      "96 MSE loss 0.070969 (0.088694)\n",
      "97 MSE loss 0.057445 (0.088676)\n",
      "98 MSE loss 0.081774 (0.088617)\n",
      "99 MSE loss 0.112269 (0.088630)\n",
      "100 MSE loss 0.076023 (0.088625)\n",
      "101 MSE loss 0.075753 (0.088612)\n",
      "102 MSE loss 0.070390 (0.088568)\n",
      "103 MSE loss 0.114754 (0.088525)\n",
      "104 MSE loss 0.097080 (0.088562)\n",
      "105 MSE loss 0.093934 (0.088544)\n",
      "106 MSE loss 0.060566 (0.088521)\n",
      "107 MSE loss 0.071230 (0.088510)\n",
      "108 MSE loss 0.069559 (0.088446)\n",
      "109 MSE loss 0.066879 (0.088438)\n",
      "110 MSE loss 0.089648 (0.088412)\n",
      "111 MSE loss 0.079424 (0.088466)\n",
      "112 MSE loss 0.080037 (0.088463)\n",
      "113 MSE loss 0.081832 (0.088398)\n",
      "114 MSE loss 0.075385 (0.088435)\n",
      "115 MSE loss 0.072287 (0.088382)\n",
      "116 MSE loss 0.067131 (0.088341)\n",
      "117 MSE loss 0.073908 (0.088287)\n",
      "118 MSE loss 0.048736 (0.088274)\n",
      "119 MSE loss 0.087920 (0.088250)\n",
      "120 MSE loss 0.061982 (0.088233)\n",
      "121 MSE loss 0.063482 (0.088200)\n",
      "122 MSE loss 0.058366 (0.088165)\n",
      "123 MSE loss 0.078983 (0.088143)\n",
      "124 MSE loss 0.062148 (0.088154)\n",
      "125 MSE loss 0.078226 (0.088116)\n",
      "126 MSE loss 0.081851 (0.088094)\n",
      "127 MSE loss 0.062020 (0.088063)\n",
      "128 MSE loss 0.098428 (0.088015)\n",
      "129 MSE loss 0.081486 (0.087990)\n",
      "130 MSE loss 0.097341 (0.087957)\n",
      "131 MSE loss 0.067164 (0.087936)\n",
      "132 MSE loss 0.090973 (0.087904)\n",
      "133 MSE loss 0.060479 (0.087885)\n",
      "134 MSE loss 0.062296 (0.087878)\n",
      "135 MSE loss 0.063192 (0.087862)\n",
      "136 MSE loss 0.088850 (0.087847)\n",
      "137 MSE loss 0.090854 (0.087855)\n",
      "138 MSE loss 0.072080 (0.087816)\n",
      "139 MSE loss 0.073396 (0.087816)\n",
      "140 MSE loss 0.070759 (0.087798)\n",
      "141 MSE loss 0.057875 (0.087797)\n",
      "142 MSE loss 0.043441 (0.087760)\n",
      "143 MSE loss 0.060460 (0.087701)\n",
      "144 MSE loss 0.066704 (0.087685)\n",
      "145 MSE loss 0.076593 (0.087649)\n",
      "146 MSE loss 0.065057 (0.087623)\n",
      "147 MSE loss 0.092044 (0.087607)\n",
      "148 MSE loss 0.092193 (0.087566)\n",
      "149 MSE loss 0.096776 (0.087550)\n",
      "150 MSE loss 0.084723 (0.087533)\n",
      "151 MSE loss 0.096115 (0.087531)\n",
      "152 MSE loss 0.065311 (0.087509)\n",
      "153 MSE loss 0.072638 (0.087522)\n",
      "154 MSE loss 0.124709 (0.087509)\n",
      "155 MSE loss 0.064306 (0.087501)\n",
      "156 MSE loss 0.059543 (0.087457)\n",
      "157 MSE loss 0.058387 (0.087429)\n",
      "158 MSE loss 0.056589 (0.087437)\n",
      "159 MSE loss 0.074728 (0.087434)\n",
      "160 MSE loss 0.073198 (0.087416)\n",
      "161 MSE loss 0.069058 (0.087392)\n",
      "162 MSE loss 0.091363 (0.087357)\n",
      "163 MSE loss 0.093336 (0.087352)\n",
      "164 MSE loss 0.075038 (0.087352)\n",
      "165 MSE loss 0.071337 (0.087335)\n",
      "166 MSE loss 0.085842 (0.087321)\n",
      "167 MSE loss 0.069404 (0.087296)\n",
      "168 MSE loss 0.081186 (0.087246)\n",
      "169 MSE loss 0.071728 (0.087228)\n",
      "170 MSE loss 0.082497 (0.087214)\n",
      "171 MSE loss 0.112817 (0.087197)\n",
      "172 MSE loss 0.068874 (0.087206)\n",
      "173 MSE loss 0.072384 (0.087204)\n",
      "174 MSE loss 0.071280 (0.087201)\n",
      "175 MSE loss 0.049409 (0.087178)\n",
      "176 MSE loss 0.080648 (0.087142)\n",
      "177 MSE loss 0.078553 (0.087125)\n",
      "178 MSE loss 0.106268 (0.087114)\n",
      "179 MSE loss 0.053792 (0.087099)\n",
      "180 MSE loss 0.053932 (0.087105)\n",
      "181 MSE loss 0.071829 (0.087071)\n",
      "182 MSE loss 0.093386 (0.087071)\n",
      "183 MSE loss 0.082475 (0.087070)\n",
      "184 MSE loss 0.090925 (0.087085)\n",
      "185 MSE loss 0.065028 (0.087098)\n",
      "186 MSE loss 0.089628 (0.087084)\n",
      "187 MSE loss 0.055378 (0.087054)\n",
      "188 MSE loss 0.069861 (0.087028)\n",
      "189 MSE loss 0.063393 (0.086991)\n",
      "190 MSE loss 0.072056 (0.086981)\n",
      "191 MSE loss 0.062904 (0.086958)\n",
      "192 MSE loss 0.066328 (0.086947)\n",
      "193 MSE loss 0.057704 (0.086924)\n",
      "194 MSE loss 0.090773 (0.086903)\n",
      "195 MSE loss 0.109139 (0.086907)\n",
      "196 MSE loss 0.077902 (0.086888)\n",
      "197 MSE loss 0.067197 (0.086873)\n",
      "198 MSE loss 0.063354 (0.086858)\n",
      "199 MSE loss 0.081781 (0.086826)\n",
      "200 MSE loss 0.087925 (0.086812)\n",
      "201 MSE loss 0.072268 (0.086799)\n",
      "202 MSE loss 0.088995 (0.086775)\n",
      "203 MSE loss 0.066425 (0.086739)\n",
      "204 MSE loss 0.064323 (0.086726)\n",
      "205 MSE loss 0.074073 (0.086718)\n",
      "206 MSE loss 0.074369 (0.086697)\n",
      "207 MSE loss 0.067161 (0.086671)\n",
      "208 MSE loss 0.087820 (0.086673)\n",
      "209 MSE loss 0.099530 (0.086641)\n",
      "210 MSE loss 0.070673 (0.086632)\n",
      "211 MSE loss 0.070115 (0.086606)\n",
      "212 MSE loss 0.085853 (0.086574)\n",
      "213 MSE loss 0.080860 (0.086564)\n",
      "214 MSE loss 0.066601 (0.086544)\n",
      "215 MSE loss 0.096843 (0.086509)\n",
      "216 MSE loss 0.068200 (0.086496)\n",
      "217 MSE loss 0.062289 (0.086474)\n",
      "218 MSE loss 0.088077 (0.086441)\n",
      "219 MSE loss 0.073683 (0.086419)\n",
      "220 MSE loss 0.088355 (0.086397)\n",
      "221 MSE loss 0.078975 (0.086389)\n",
      "222 MSE loss 0.081270 (0.086387)\n",
      "223 MSE loss 0.063124 (0.086380)\n",
      "224 MSE loss 0.077305 (0.086363)\n",
      "225 MSE loss 0.064848 (0.086351)\n",
      "226 MSE loss 0.079748 (0.086344)\n",
      "227 MSE loss 0.072540 (0.086328)\n",
      "228 MSE loss 0.077103 (0.086309)\n",
      "229 MSE loss 0.078664 (0.086298)\n",
      "230 MSE loss 0.047895 (0.086287)\n",
      "231 MSE loss 0.073061 (0.086268)\n",
      "232 MSE loss 0.091077 (0.086257)\n",
      "233 MSE loss 0.064182 (0.086254)\n",
      "234 MSE loss 0.074297 (0.086248)\n",
      "235 MSE loss 0.110804 (0.086235)\n",
      "236 MSE loss 0.098012 (0.086236)\n",
      "237 MSE loss 0.081669 (0.086215)\n",
      "238 MSE loss 0.060569 (0.086198)\n",
      "239 MSE loss 0.081939 (0.086177)\n",
      "240 MSE loss 0.079720 (0.086146)\n",
      "241 MSE loss 0.091311 (0.086115)\n",
      "242 MSE loss 0.062220 (0.086101)\n",
      "243 MSE loss 0.068608 (0.086077)\n",
      "244 MSE loss 0.052336 (0.086065)\n",
      "245 MSE loss 0.079343 (0.086054)\n",
      "246 MSE loss 0.060229 (0.086037)\n",
      "247 MSE loss 0.065117 (0.086016)\n",
      "248 MSE loss 0.065211 (0.086006)\n",
      "249 MSE loss 0.092077 (0.085989)\n",
      "250 MSE loss 0.095749 (0.085985)\n",
      "251 MSE loss 0.092156 (0.085962)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 MSE loss 0.066721 (0.085957)\n",
      "253 MSE loss 0.091607 (0.085951)\n",
      "254 MSE loss 0.080794 (0.085941)\n",
      "255 MSE loss 0.086176 (0.085934)\n",
      "256 MSE loss 0.072434 (0.085920)\n",
      "257 MSE loss 0.089637 (0.085906)\n",
      "258 MSE loss 0.076787 (0.085888)\n",
      "259 MSE loss 0.103431 (0.085876)\n",
      "260 MSE loss 0.083105 (0.085858)\n",
      "261 MSE loss 0.076761 (0.085842)\n",
      "262 MSE loss 0.077294 (0.085839)\n",
      "263 MSE loss 0.069650 (0.085837)\n",
      "264 MSE loss 0.102120 (0.085839)\n",
      "265 MSE loss 0.079808 (0.085826)\n",
      "266 MSE loss 0.052678 (0.085808)\n",
      "267 MSE loss 0.074606 (0.085793)\n",
      "268 MSE loss 0.068792 (0.085780)\n",
      "269 MSE loss 0.077431 (0.085774)\n",
      "270 MSE loss 0.077719 (0.085751)\n",
      "271 MSE loss 0.077695 (0.085749)\n",
      "272 MSE loss 0.101784 (0.085725)\n",
      "273 MSE loss 0.110361 (0.085697)\n",
      "274 MSE loss 0.077950 (0.085683)\n",
      "275 MSE loss 0.065609 (0.085685)\n",
      "276 MSE loss 0.092710 (0.085662)\n",
      "277 MSE loss 0.061934 (0.085653)\n",
      "278 MSE loss 0.061316 (0.085649)\n",
      "279 MSE loss 0.094723 (0.085641)\n",
      "280 MSE loss 0.047068 (0.085624)\n",
      "281 MSE loss 0.080573 (0.085611)\n",
      "282 MSE loss 0.079497 (0.085596)\n",
      "283 MSE loss 0.084165 (0.085592)\n",
      "284 MSE loss 0.077642 (0.085571)\n",
      "285 MSE loss 0.085312 (0.085561)\n",
      "286 MSE loss 0.079299 (0.085560)\n",
      "287 MSE loss 0.075105 (0.085523)\n",
      "288 MSE loss 0.068028 (0.085518)\n",
      "289 MSE loss 0.085531 (0.085503)\n",
      "290 MSE loss 0.069763 (0.085498)\n",
      "291 MSE loss 0.076887 (0.085490)\n",
      "292 MSE loss 0.077691 (0.085477)\n",
      "293 MSE loss 0.078232 (0.085460)\n",
      "294 MSE loss 0.074596 (0.085434)\n",
      "295 MSE loss 0.088643 (0.085431)\n",
      "296 MSE loss 0.076127 (0.085406)\n",
      "297 MSE loss 0.062005 (0.085382)\n",
      "298 MSE loss 0.081563 (0.085367)\n",
      "299 MSE loss 0.087537 (0.085365)\n",
      "300 MSE loss 0.063902 (0.085354)\n",
      "301 MSE loss 0.070230 (0.085342)\n",
      "302 MSE loss 0.101282 (0.085342)\n",
      "303 MSE loss 0.070720 (0.085345)\n",
      "304 MSE loss 0.076108 (0.085342)\n",
      "305 MSE loss 0.071209 (0.085336)\n",
      "306 MSE loss 0.063662 (0.085331)\n",
      "307 MSE loss 0.070668 (0.085310)\n",
      "308 MSE loss 0.100422 (0.085290)\n",
      "309 MSE loss 0.079434 (0.085274)\n",
      "310 MSE loss 0.065590 (0.085256)\n",
      "311 MSE loss 0.071881 (0.085234)\n",
      "312 MSE loss 0.064763 (0.085218)\n",
      "313 MSE loss 0.077501 (0.085200)\n",
      "314 MSE loss 0.095845 (0.085186)\n",
      "315 MSE loss 0.066000 (0.085161)\n",
      "316 MSE loss 0.070645 (0.085141)\n",
      "317 MSE loss 0.058818 (0.085114)\n",
      "318 MSE loss 0.072829 (0.085109)\n",
      "319 MSE loss 0.064802 (0.085093)\n",
      "320 MSE loss 0.069586 (0.085064)\n",
      "321 MSE loss 0.112796 (0.085044)\n",
      "322 MSE loss 0.069185 (0.085030)\n",
      "323 MSE loss 0.063425 (0.085005)\n",
      "324 MSE loss 0.070427 (0.084979)\n",
      "325 MSE loss 0.068433 (0.084962)\n",
      "326 MSE loss 0.080168 (0.084946)\n",
      "327 MSE loss 0.081653 (0.084941)\n",
      "328 MSE loss 0.065220 (0.084930)\n",
      "329 MSE loss 0.068428 (0.084917)\n",
      "330 MSE loss 0.068273 (0.084895)\n",
      "331 MSE loss 0.068158 (0.084884)\n",
      "332 MSE loss 0.066008 (0.084872)\n",
      "333 MSE loss 0.044674 (0.084853)\n",
      "334 MSE loss 0.065655 (0.084826)\n",
      "335 MSE loss 0.098132 (0.084809)\n",
      "336 MSE loss 0.068003 (0.084804)\n",
      "337 MSE loss 0.077849 (0.084783)\n",
      "338 MSE loss 0.096155 (0.084760)\n",
      "339 MSE loss 0.051061 (0.084736)\n",
      "340 MSE loss 0.073761 (0.084716)\n",
      "341 MSE loss 0.091112 (0.084694)\n",
      "342 MSE loss 0.044051 (0.084676)\n",
      "343 MSE loss 0.059417 (0.084656)\n",
      "344 MSE loss 0.088466 (0.084635)\n",
      "345 MSE loss 0.085803 (0.084617)\n",
      "346 MSE loss 0.075193 (0.084594)\n",
      "347 MSE loss 0.087924 (0.084581)\n",
      "348 MSE loss 0.084608 (0.084572)\n",
      "349 MSE loss 0.093229 (0.084560)\n",
      "350 MSE loss 0.091192 (0.084556)\n",
      "351 MSE loss 0.066363 (0.084541)\n",
      "352 MSE loss 0.047807 (0.084530)\n",
      "353 MSE loss 0.061380 (0.084507)\n",
      "354 MSE loss 0.058289 (0.084487)\n",
      "355 MSE loss 0.070759 (0.084464)\n",
      "356 MSE loss 0.067782 (0.084451)\n",
      "357 MSE loss 0.074968 (0.084441)\n",
      "358 MSE loss 0.076785 (0.084433)\n",
      "359 MSE loss 0.087590 (0.084425)\n",
      "360 MSE loss 0.074631 (0.084404)\n",
      "361 MSE loss 0.066678 (0.084385)\n",
      "362 MSE loss 0.057742 (0.084364)\n",
      "363 MSE loss 0.076226 (0.084359)\n",
      "364 MSE loss 0.053833 (0.084338)\n",
      "365 MSE loss 0.077779 (0.084318)\n",
      "366 MSE loss 0.058989 (0.084305)\n",
      "367 MSE loss 0.071063 (0.084294)\n",
      "368 MSE loss 0.073024 (0.084289)\n",
      "369 MSE loss 0.073406 (0.084265)\n",
      "370 MSE loss 0.081518 (0.084248)\n",
      "371 MSE loss 0.085891 (0.084234)\n",
      "372 MSE loss 0.071066 (0.084216)\n",
      "373 MSE loss 0.069729 (0.084200)\n",
      "374 MSE loss 0.074735 (0.084194)\n",
      "375 MSE loss 0.064559 (0.084182)\n",
      "376 MSE loss 0.060029 (0.084172)\n",
      "377 MSE loss 0.068486 (0.084157)\n",
      "378 MSE loss 0.070637 (0.084152)\n",
      "379 MSE loss 0.053817 (0.084129)\n",
      "380 MSE loss 0.073838 (0.084109)\n",
      "381 MSE loss 0.067822 (0.084097)\n",
      "382 MSE loss 0.051944 (0.084071)\n",
      "383 MSE loss 0.058573 (0.084061)\n",
      "384 MSE loss 0.084205 (0.084044)\n",
      "385 MSE loss 0.061589 (0.084028)\n",
      "386 MSE loss 0.075792 (0.084009)\n",
      "387 MSE loss 0.092504 (0.083994)\n",
      "388 MSE loss 0.093702 (0.083977)\n",
      "389 MSE loss 0.068372 (0.083965)\n",
      "390 MSE loss 0.099845 (0.083953)\n",
      "391 MSE loss 0.066504 (0.083933)\n",
      "392 MSE loss 0.059892 (0.083918)\n",
      "393 MSE loss 0.081215 (0.083925)\n",
      "394 MSE loss 0.047708 (0.083909)\n",
      "395 MSE loss 0.075805 (0.083902)\n",
      "396 MSE loss 0.074411 (0.083884)\n",
      "397 MSE loss 0.064348 (0.083866)\n",
      "398 MSE loss 0.067730 (0.083858)\n",
      "399 MSE loss 0.078470 (0.083840)\n",
      "400 MSE loss 0.056392 (0.083827)\n",
      "401 MSE loss 0.059007 (0.083809)\n",
      "402 MSE loss 0.073553 (0.083793)\n",
      "403 MSE loss 0.089639 (0.083770)\n",
      "404 MSE loss 0.079675 (0.083754)\n",
      "405 MSE loss 0.073343 (0.083735)\n",
      "406 MSE loss 0.068989 (0.083726)\n",
      "407 MSE loss 0.057938 (0.083708)\n",
      "408 MSE loss 0.070158 (0.083693)\n",
      "409 MSE loss 0.074762 (0.083684)\n",
      "410 MSE loss 0.069171 (0.083672)\n",
      "411 MSE loss 0.046921 (0.083660)\n",
      "412 MSE loss 0.092395 (0.083640)\n",
      "413 MSE loss 0.089784 (0.083616)\n",
      "414 MSE loss 0.063041 (0.083605)\n",
      "415 MSE loss 0.049916 (0.083586)\n",
      "416 MSE loss 0.061764 (0.083565)\n",
      "417 MSE loss 0.069968 (0.083551)\n",
      "418 MSE loss 0.072463 (0.083528)\n",
      "419 MSE loss 0.049912 (0.083507)\n",
      "420 MSE loss 0.048299 (0.083493)\n",
      "421 MSE loss 0.097880 (0.083481)\n",
      "422 MSE loss 0.082977 (0.083469)\n",
      "423 MSE loss 0.084426 (0.083453)\n",
      "424 MSE loss 0.057681 (0.083439)\n",
      "425 MSE loss 0.056030 (0.083428)\n",
      "426 MSE loss 0.042656 (0.083408)\n",
      "427 MSE loss 0.070422 (0.083389)\n",
      "428 MSE loss 0.058768 (0.083371)\n",
      "429 MSE loss 0.101362 (0.083358)\n",
      "430 MSE loss 0.065325 (0.083345)\n",
      "431 MSE loss 0.052984 (0.083334)\n",
      "432 MSE loss 0.072915 (0.083321)\n",
      "433 MSE loss 0.052938 (0.083304)\n",
      "434 MSE loss 0.062665 (0.083287)\n",
      "435 MSE loss 0.093850 (0.083263)\n",
      "436 MSE loss 0.056351 (0.083243)\n",
      "437 MSE loss 0.058864 (0.083228)\n",
      "438 MSE loss 0.056251 (0.083216)\n",
      "439 MSE loss 0.051302 (0.083197)\n",
      "440 MSE loss 0.052307 (0.083182)\n",
      "441 MSE loss 0.053059 (0.083156)\n",
      "442 MSE loss 0.056421 (0.083144)\n",
      "443 MSE loss 0.075772 (0.083131)\n",
      "444 MSE loss 0.076123 (0.083109)\n",
      "445 MSE loss 0.053400 (0.083087)\n",
      "446 MSE loss 0.079129 (0.083069)\n",
      "447 MSE loss 0.073400 (0.083055)\n",
      "448 MSE loss 0.081436 (0.083036)\n",
      "449 MSE loss 0.059311 (0.083029)\n",
      "450 MSE loss 0.055980 (0.083018)\n",
      "451 MSE loss 0.074458 (0.082997)\n",
      "452 MSE loss 0.106228 (0.082981)\n",
      "453 MSE loss 0.064954 (0.082963)\n",
      "454 MSE loss 0.078546 (0.082944)\n",
      "455 MSE loss 0.067044 (0.082942)\n",
      "456 MSE loss 0.066379 (0.082937)\n",
      "457 MSE loss 0.057021 (0.082926)\n",
      "458 MSE loss 0.100848 (0.082918)\n",
      "459 MSE loss 0.087916 (0.082904)\n",
      "460 MSE loss 0.087812 (0.082890)\n",
      "461 MSE loss 0.090259 (0.082880)\n",
      "462 MSE loss 0.096002 (0.082866)\n",
      "463 MSE loss 0.082649 (0.082853)\n",
      "464 MSE loss 0.084354 (0.082843)\n",
      "465 MSE loss 0.065254 (0.082828)\n",
      "466 MSE loss 0.059858 (0.082818)\n",
      "467 MSE loss 0.078493 (0.082810)\n",
      "468 MSE loss 0.067748 (0.082796)\n",
      "469 MSE loss 0.086956 (0.082785)\n",
      "470 MSE loss 0.063791 (0.082775)\n",
      "471 MSE loss 0.080947 (0.082754)\n",
      "472 MSE loss 0.079452 (0.082741)\n",
      "473 MSE loss 0.064871 (0.082738)\n",
      "474 MSE loss 0.062234 (0.082727)\n",
      "475 MSE loss 0.080853 (0.082717)\n",
      "476 MSE loss 0.109671 (0.082704)\n",
      "477 MSE loss 0.068769 (0.082693)\n",
      "478 MSE loss 0.084541 (0.082675)\n",
      "479 MSE loss 0.082473 (0.082660)\n",
      "480 MSE loss 0.056940 (0.082645)\n",
      "481 MSE loss 0.107027 (0.082631)\n",
      "482 MSE loss 0.068048 (0.082623)\n",
      "483 MSE loss 0.061686 (0.082603)\n",
      "484 MSE loss 0.078000 (0.082594)\n",
      "485 MSE loss 0.042815 (0.082580)\n",
      "486 MSE loss 0.083303 (0.082567)\n",
      "487 MSE loss 0.083948 (0.082566)\n",
      "488 MSE loss 0.098888 (0.082559)\n",
      "489 MSE loss 0.040226 (0.082547)\n",
      "490 MSE loss 0.068022 (0.082535)\n",
      "491 MSE loss 0.043821 (0.082514)\n",
      "492 MSE loss 0.079234 (0.082500)\n",
      "493 MSE loss 0.069178 (0.082493)\n",
      "494 MSE loss 0.044804 (0.082480)\n",
      "495 MSE loss 0.070894 (0.082473)\n",
      "496 MSE loss 0.071373 (0.082462)\n",
      "497 MSE loss 0.053551 (0.082452)\n",
      "498 MSE loss 0.051185 (0.082433)\n",
      "499 MSE loss 0.083582 (0.082419)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79d8bf73cac42a4b2eb898f255d1eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.106643 (0.106298)\n",
      "1 MSE loss 0.113943 (0.106718)\n",
      "2 MSE loss 0.074073 (0.104043)\n",
      "3 MSE loss 0.113611 (0.105167)\n",
      "4 MSE loss 0.074231 (0.105024)\n",
      "5 MSE loss 0.112605 (0.105276)\n",
      "6 MSE loss 0.079409 (0.105300)\n",
      "7 MSE loss 0.089657 (0.104515)\n",
      "8 MSE loss 0.116252 (0.104670)\n",
      "9 MSE loss 0.100385 (0.104546)\n",
      "10 MSE loss 0.099125 (0.104299)\n",
      "11 MSE loss 0.118984 (0.104060)\n",
      "12 MSE loss 0.086485 (0.103860)\n",
      "13 MSE loss 0.081786 (0.103658)\n",
      "14 MSE loss 0.107867 (0.103807)\n",
      "15 MSE loss 0.125315 (0.103903)\n",
      "16 MSE loss 0.100561 (0.103910)\n",
      "17 MSE loss 0.105635 (0.103742)\n",
      "18 MSE loss 0.084244 (0.103663)\n",
      "19 MSE loss 0.115845 (0.103603)\n",
      "20 MSE loss 0.068527 (0.103813)\n",
      "21 MSE loss 0.100496 (0.103860)\n",
      "22 MSE loss 0.108237 (0.103730)\n",
      "23 MSE loss 0.089182 (0.103739)\n",
      "24 MSE loss 0.076604 (0.103687)\n",
      "25 MSE loss 0.115721 (0.103647)\n",
      "26 MSE loss 0.079068 (0.103634)\n",
      "27 MSE loss 0.107329 (0.103671)\n",
      "28 MSE loss 0.117034 (0.103613)\n",
      "29 MSE loss 0.076375 (0.103592)\n",
      "30 MSE loss 0.135216 (0.103543)\n",
      "31 MSE loss 0.077994 (0.103483)\n",
      "32 MSE loss 0.086898 (0.103383)\n",
      "33 MSE loss 0.072265 (0.103324)\n",
      "34 MSE loss 0.090098 (0.103210)\n",
      "35 MSE loss 0.102855 (0.103173)\n",
      "36 MSE loss 0.070987 (0.102987)\n",
      "37 MSE loss 0.098833 (0.102948)\n",
      "38 MSE loss 0.095927 (0.102910)\n",
      "39 MSE loss 0.114171 (0.102870)\n",
      "40 MSE loss 0.100300 (0.102750)\n",
      "41 MSE loss 0.102354 (0.102652)\n",
      "42 MSE loss 0.107770 (0.102610)\n",
      "43 MSE loss 0.109478 (0.102580)\n",
      "44 MSE loss 0.087445 (0.102573)\n",
      "45 MSE loss 0.105326 (0.102538)\n",
      "46 MSE loss 0.088530 (0.102543)\n",
      "47 MSE loss 0.104375 (0.102536)\n",
      "48 MSE loss 0.071060 (0.102456)\n",
      "49 MSE loss 0.099577 (0.102439)\n",
      "50 MSE loss 0.121137 (0.102384)\n",
      "51 MSE loss 0.104339 (0.102230)\n",
      "52 MSE loss 0.094970 (0.102228)\n",
      "53 MSE loss 0.098270 (0.102223)\n",
      "54 MSE loss 0.082330 (0.102160)\n",
      "55 MSE loss 0.117115 (0.102010)\n",
      "56 MSE loss 0.079697 (0.101947)\n",
      "57 MSE loss 0.100952 (0.101946)\n",
      "58 MSE loss 0.071593 (0.102020)\n",
      "59 MSE loss 0.087089 (0.101986)\n",
      "60 MSE loss 0.088114 (0.101969)\n",
      "61 MSE loss 0.097699 (0.101951)\n",
      "62 MSE loss 0.081796 (0.101963)\n",
      "63 MSE loss 0.084414 (0.101968)\n",
      "64 MSE loss 0.086337 (0.101934)\n",
      "65 MSE loss 0.091391 (0.101867)\n",
      "66 MSE loss 0.097933 (0.101817)\n",
      "67 MSE loss 0.084700 (0.101770)\n",
      "68 MSE loss 0.091875 (0.101677)\n",
      "69 MSE loss 0.089002 (0.101681)\n",
      "70 MSE loss 0.106996 (0.101634)\n",
      "71 MSE loss 0.087848 (0.101614)\n",
      "72 MSE loss 0.119420 (0.101589)\n",
      "73 MSE loss 0.089059 (0.101573)\n",
      "74 MSE loss 0.105750 (0.101649)\n",
      "75 MSE loss 0.088980 (0.101644)\n",
      "76 MSE loss 0.098184 (0.101661)\n",
      "77 MSE loss 0.079122 (0.101658)\n",
      "78 MSE loss 0.104026 (0.101565)\n",
      "79 MSE loss 0.098054 (0.101533)\n",
      "80 MSE loss 0.070428 (0.101535)\n",
      "81 MSE loss 0.082341 (0.101460)\n",
      "82 MSE loss 0.085063 (0.101376)\n",
      "83 MSE loss 0.091275 (0.101382)\n",
      "84 MSE loss 0.068808 (0.101340)\n",
      "85 MSE loss 0.124033 (0.101291)\n",
      "86 MSE loss 0.110257 (0.101244)\n",
      "87 MSE loss 0.085658 (0.101246)\n",
      "88 MSE loss 0.079277 (0.101274)\n",
      "89 MSE loss 0.082410 (0.101274)\n",
      "90 MSE loss 0.094851 (0.101229)\n",
      "91 MSE loss 0.088751 (0.101172)\n",
      "92 MSE loss 0.069744 (0.101126)\n",
      "93 MSE loss 0.083191 (0.101091)\n",
      "94 MSE loss 0.079806 (0.101036)\n",
      "95 MSE loss 0.083225 (0.101011)\n",
      "96 MSE loss 0.069951 (0.100965)\n",
      "97 MSE loss 0.085191 (0.100894)\n",
      "98 MSE loss 0.072341 (0.100901)\n",
      "99 MSE loss 0.123242 (0.100872)\n",
      "100 MSE loss 0.108406 (0.100860)\n",
      "101 MSE loss 0.085191 (0.100853)\n",
      "102 MSE loss 0.108123 (0.100822)\n",
      "103 MSE loss 0.073110 (0.100811)\n",
      "104 MSE loss 0.100752 (0.100754)\n",
      "105 MSE loss 0.089490 (0.100704)\n",
      "106 MSE loss 0.087605 (0.100690)\n",
      "107 MSE loss 0.089000 (0.100644)\n",
      "108 MSE loss 0.087218 (0.100598)\n",
      "109 MSE loss 0.093313 (0.100558)\n",
      "110 MSE loss 0.079617 (0.100547)\n",
      "111 MSE loss 0.075025 (0.100513)\n",
      "112 MSE loss 0.099459 (0.100478)\n",
      "113 MSE loss 0.116523 (0.100447)\n",
      "114 MSE loss 0.078028 (0.100390)\n",
      "115 MSE loss 0.093109 (0.100344)\n",
      "116 MSE loss 0.079111 (0.100323)\n",
      "117 MSE loss 0.102123 (0.100303)\n",
      "118 MSE loss 0.083488 (0.100231)\n",
      "119 MSE loss 0.094722 (0.100195)\n",
      "120 MSE loss 0.099198 (0.100174)\n",
      "121 MSE loss 0.103510 (0.100105)\n",
      "122 MSE loss 0.082737 (0.100069)\n",
      "123 MSE loss 0.095611 (0.100041)\n",
      "124 MSE loss 0.081803 (0.099980)\n",
      "125 MSE loss 0.100905 (0.099989)\n",
      "126 MSE loss 0.084317 (0.099940)\n",
      "127 MSE loss 0.060637 (0.099871)\n",
      "128 MSE loss 0.075068 (0.099834)\n",
      "129 MSE loss 0.080596 (0.099820)\n",
      "130 MSE loss 0.081703 (0.099778)\n",
      "131 MSE loss 0.064777 (0.099721)\n",
      "132 MSE loss 0.055451 (0.099711)\n",
      "133 MSE loss 0.093163 (0.099706)\n",
      "134 MSE loss 0.084024 (0.099685)\n",
      "135 MSE loss 0.096646 (0.099644)\n",
      "136 MSE loss 0.099999 (0.099624)\n",
      "137 MSE loss 0.099393 (0.099589)\n",
      "138 MSE loss 0.076282 (0.099570)\n",
      "139 MSE loss 0.055736 (0.099536)\n",
      "140 MSE loss 0.121321 (0.099482)\n",
      "141 MSE loss 0.072051 (0.099480)\n",
      "142 MSE loss 0.086629 (0.099445)\n",
      "143 MSE loss 0.086854 (0.099450)\n",
      "144 MSE loss 0.078488 (0.099431)\n",
      "145 MSE loss 0.053415 (0.099380)\n",
      "146 MSE loss 0.087027 (0.099360)\n",
      "147 MSE loss 0.086389 (0.099344)\n",
      "148 MSE loss 0.078960 (0.099303)\n",
      "149 MSE loss 0.093565 (0.099285)\n",
      "150 MSE loss 0.105502 (0.099239)\n",
      "151 MSE loss 0.076681 (0.099200)\n",
      "152 MSE loss 0.092919 (0.099171)\n",
      "153 MSE loss 0.063236 (0.099115)\n",
      "154 MSE loss 0.079418 (0.099053)\n",
      "155 MSE loss 0.106652 (0.099042)\n",
      "156 MSE loss 0.102647 (0.098982)\n",
      "157 MSE loss 0.120774 (0.098950)\n",
      "158 MSE loss 0.083990 (0.098914)\n",
      "159 MSE loss 0.094797 (0.098896)\n",
      "160 MSE loss 0.093849 (0.098863)\n",
      "161 MSE loss 0.109649 (0.098844)\n",
      "162 MSE loss 0.086054 (0.098818)\n",
      "163 MSE loss 0.091812 (0.098777)\n",
      "164 MSE loss 0.088705 (0.098758)\n",
      "165 MSE loss 0.085153 (0.098729)\n",
      "166 MSE loss 0.091966 (0.098730)\n",
      "167 MSE loss 0.091265 (0.098678)\n",
      "168 MSE loss 0.113360 (0.098662)\n",
      "169 MSE loss 0.087389 (0.098638)\n",
      "170 MSE loss 0.073960 (0.098622)\n",
      "171 MSE loss 0.114580 (0.098581)\n",
      "172 MSE loss 0.147346 (0.098578)\n",
      "173 MSE loss 0.082067 (0.098573)\n",
      "174 MSE loss 0.087870 (0.098524)\n",
      "175 MSE loss 0.074446 (0.098495)\n",
      "176 MSE loss 0.099833 (0.098444)\n",
      "177 MSE loss 0.093541 (0.098424)\n",
      "178 MSE loss 0.068909 (0.098395)\n",
      "179 MSE loss 0.110859 (0.098382)\n",
      "180 MSE loss 0.070389 (0.098376)\n",
      "181 MSE loss 0.064917 (0.098364)\n",
      "182 MSE loss 0.099595 (0.098351)\n",
      "183 MSE loss 0.088841 (0.098337)\n",
      "184 MSE loss 0.069037 (0.098299)\n",
      "185 MSE loss 0.066651 (0.098264)\n",
      "186 MSE loss 0.081230 (0.098230)\n",
      "187 MSE loss 0.103037 (0.098227)\n",
      "188 MSE loss 0.078993 (0.098190)\n",
      "189 MSE loss 0.098237 (0.098148)\n",
      "190 MSE loss 0.100400 (0.098146)\n",
      "191 MSE loss 0.098030 (0.098121)\n",
      "192 MSE loss 0.112860 (0.098115)\n",
      "193 MSE loss 0.101826 (0.098105)\n",
      "194 MSE loss 0.110407 (0.098055)\n",
      "195 MSE loss 0.096634 (0.098039)\n",
      "196 MSE loss 0.105401 (0.098009)\n",
      "197 MSE loss 0.070306 (0.097970)\n",
      "198 MSE loss 0.119526 (0.097939)\n",
      "199 MSE loss 0.098308 (0.097933)\n",
      "200 MSE loss 0.077815 (0.097901)\n",
      "201 MSE loss 0.091434 (0.097880)\n",
      "202 MSE loss 0.065471 (0.097837)\n",
      "203 MSE loss 0.078009 (0.097802)\n",
      "204 MSE loss 0.108721 (0.097773)\n",
      "205 MSE loss 0.104981 (0.097766)\n",
      "206 MSE loss 0.090333 (0.097716)\n",
      "207 MSE loss 0.119227 (0.097701)\n",
      "208 MSE loss 0.096020 (0.097660)\n",
      "209 MSE loss 0.100391 (0.097632)\n",
      "210 MSE loss 0.075458 (0.097626)\n",
      "211 MSE loss 0.076134 (0.097617)\n",
      "212 MSE loss 0.098316 (0.097581)\n",
      "213 MSE loss 0.092780 (0.097569)\n",
      "214 MSE loss 0.095822 (0.097551)\n",
      "215 MSE loss 0.100132 (0.097512)\n",
      "216 MSE loss 0.087026 (0.097527)\n",
      "217 MSE loss 0.094591 (0.097523)\n",
      "218 MSE loss 0.101175 (0.097513)\n",
      "219 MSE loss 0.060389 (0.097476)\n",
      "220 MSE loss 0.075396 (0.097436)\n",
      "221 MSE loss 0.081776 (0.097432)\n",
      "222 MSE loss 0.100852 (0.097421)\n",
      "223 MSE loss 0.078904 (0.097412)\n",
      "224 MSE loss 0.089847 (0.097368)\n",
      "225 MSE loss 0.083579 (0.097338)\n",
      "226 MSE loss 0.082452 (0.097299)\n",
      "227 MSE loss 0.068062 (0.097290)\n",
      "228 MSE loss 0.085388 (0.097266)\n",
      "229 MSE loss 0.061351 (0.097219)\n",
      "230 MSE loss 0.079300 (0.097183)\n",
      "231 MSE loss 0.088663 (0.097176)\n",
      "232 MSE loss 0.100133 (0.097152)\n",
      "233 MSE loss 0.068589 (0.097119)\n",
      "234 MSE loss 0.066931 (0.097081)\n",
      "235 MSE loss 0.098819 (0.097048)\n",
      "236 MSE loss 0.074789 (0.097014)\n",
      "237 MSE loss 0.073169 (0.096983)\n",
      "238 MSE loss 0.077988 (0.096956)\n",
      "239 MSE loss 0.075922 (0.096933)\n",
      "240 MSE loss 0.104825 (0.096910)\n",
      "241 MSE loss 0.061623 (0.096876)\n",
      "242 MSE loss 0.111185 (0.096871)\n",
      "243 MSE loss 0.070489 (0.096851)\n",
      "244 MSE loss 0.093019 (0.096822)\n",
      "245 MSE loss 0.068652 (0.096788)\n",
      "246 MSE loss 0.096295 (0.096739)\n",
      "247 MSE loss 0.077140 (0.096711)\n",
      "248 MSE loss 0.096373 (0.096695)\n",
      "249 MSE loss 0.087058 (0.096677)\n",
      "250 MSE loss 0.081970 (0.096655)\n",
      "251 MSE loss 0.094054 (0.096637)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 MSE loss 0.083716 (0.096610)\n",
      "253 MSE loss 0.078405 (0.096574)\n",
      "254 MSE loss 0.085811 (0.096563)\n",
      "255 MSE loss 0.097454 (0.096542)\n",
      "256 MSE loss 0.099524 (0.096526)\n",
      "257 MSE loss 0.110325 (0.096511)\n",
      "258 MSE loss 0.088997 (0.096497)\n",
      "259 MSE loss 0.069893 (0.096476)\n",
      "260 MSE loss 0.083241 (0.096458)\n",
      "261 MSE loss 0.080231 (0.096444)\n",
      "262 MSE loss 0.094007 (0.096410)\n",
      "263 MSE loss 0.077578 (0.096386)\n",
      "264 MSE loss 0.098622 (0.096365)\n",
      "265 MSE loss 0.081704 (0.096337)\n",
      "266 MSE loss 0.095194 (0.096318)\n",
      "267 MSE loss 0.070054 (0.096299)\n",
      "268 MSE loss 0.079418 (0.096270)\n",
      "269 MSE loss 0.069480 (0.096241)\n",
      "270 MSE loss 0.113601 (0.096208)\n",
      "271 MSE loss 0.106222 (0.096188)\n",
      "272 MSE loss 0.100913 (0.096184)\n",
      "273 MSE loss 0.095209 (0.096177)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04365f29deaf48e5a46a657e7afdc646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.096007 (0.110890)\n",
      "1 MSE loss 0.072067 (0.108965)\n",
      "2 MSE loss 0.146556 (0.108128)\n",
      "3 MSE loss 0.100473 (0.107882)\n",
      "4 MSE loss 0.074850 (0.107494)\n",
      "5 MSE loss 0.095298 (0.107381)\n",
      "6 MSE loss 0.106468 (0.107113)\n",
      "7 MSE loss 0.088696 (0.106921)\n",
      "8 MSE loss 0.099535 (0.106740)\n",
      "9 MSE loss 0.105636 (0.107010)\n",
      "10 MSE loss 0.093375 (0.106748)\n",
      "11 MSE loss 0.086244 (0.107040)\n",
      "12 MSE loss 0.097365 (0.106830)\n",
      "13 MSE loss 0.085333 (0.106546)\n",
      "14 MSE loss 0.133335 (0.106289)\n",
      "15 MSE loss 0.071829 (0.105721)\n",
      "16 MSE loss 0.097044 (0.105745)\n",
      "17 MSE loss 0.085159 (0.105642)\n",
      "18 MSE loss 0.089149 (0.105373)\n",
      "19 MSE loss 0.114895 (0.105685)\n",
      "20 MSE loss 0.105325 (0.105439)\n",
      "21 MSE loss 0.095657 (0.105426)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286d536edd5b4b898b63018325d54d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.099897 (0.112900)\n",
      "1 MSE loss 0.100994 (0.111993)\n",
      "2 MSE loss 0.102970 (0.109686)\n",
      "3 MSE loss 0.140825 (0.109538)\n",
      "4 MSE loss 0.129591 (0.109756)\n",
      "5 MSE loss 0.084102 (0.109183)\n",
      "6 MSE loss 0.126795 (0.110120)\n",
      "7 MSE loss 0.096781 (0.109807)\n",
      "8 MSE loss 0.081629 (0.109642)\n",
      "9 MSE loss 0.104048 (0.109707)\n",
      "10 MSE loss 0.086066 (0.109476)\n",
      "11 MSE loss 0.098593 (0.109336)\n",
      "12 MSE loss 0.096278 (0.109107)\n",
      "13 MSE loss 0.105085 (0.108954)\n",
      "14 MSE loss 0.125291 (0.108750)\n",
      "15 MSE loss 0.097528 (0.108672)\n",
      "16 MSE loss 0.094768 (0.108490)\n",
      "17 MSE loss 0.103927 (0.108358)\n",
      "18 MSE loss 0.082545 (0.108298)\n",
      "19 MSE loss 0.132447 (0.108224)\n",
      "20 MSE loss 0.086068 (0.108086)\n",
      "21 MSE loss 0.108391 (0.108227)\n",
      "22 MSE loss 0.090835 (0.108252)\n",
      "23 MSE loss 0.105551 (0.108110)\n",
      "24 MSE loss 0.103209 (0.108100)\n",
      "25 MSE loss 0.102756 (0.108134)\n",
      "26 MSE loss 0.107961 (0.108229)\n",
      "27 MSE loss 0.105713 (0.108237)\n",
      "28 MSE loss 0.113743 (0.108273)\n",
      "29 MSE loss 0.125580 (0.108159)\n",
      "30 MSE loss 0.099502 (0.108180)\n",
      "31 MSE loss 0.092203 (0.108053)\n",
      "32 MSE loss 0.104078 (0.108006)\n",
      "33 MSE loss 0.092898 (0.107969)\n",
      "34 MSE loss 0.088372 (0.108038)\n",
      "35 MSE loss 0.097343 (0.107900)\n",
      "36 MSE loss 0.114481 (0.107832)\n",
      "37 MSE loss 0.105845 (0.107782)\n",
      "38 MSE loss 0.088403 (0.107885)\n",
      "39 MSE loss 0.087202 (0.107902)\n",
      "40 MSE loss 0.100945 (0.107848)\n",
      "41 MSE loss 0.105199 (0.107832)\n",
      "42 MSE loss 0.113619 (0.107582)\n",
      "43 MSE loss 0.144591 (0.107403)\n",
      "44 MSE loss 0.121490 (0.107412)\n",
      "45 MSE loss 0.106936 (0.107334)\n",
      "46 MSE loss 0.097984 (0.107221)\n",
      "47 MSE loss 0.091770 (0.107213)\n",
      "48 MSE loss 0.101309 (0.107200)\n",
      "49 MSE loss 0.094138 (0.107201)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ced8440d424dc2973407c366501690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.105325 (0.118253)\n",
      "1 MSE loss 0.111741 (0.116479)\n",
      "2 MSE loss 0.101319 (0.115494)\n",
      "3 MSE loss 0.098992 (0.115162)\n",
      "4 MSE loss 0.133094 (0.115451)\n",
      "5 MSE loss 0.086618 (0.115452)\n",
      "6 MSE loss 0.093234 (0.115139)\n",
      "7 MSE loss 0.096003 (0.115391)\n",
      "8 MSE loss 0.097126 (0.114990)\n",
      "9 MSE loss 0.104535 (0.114809)\n",
      "10 MSE loss 0.093854 (0.114404)\n",
      "11 MSE loss 0.085148 (0.114135)\n",
      "12 MSE loss 0.112327 (0.113666)\n",
      "13 MSE loss 0.092982 (0.113517)\n",
      "14 MSE loss 0.121986 (0.113143)\n",
      "15 MSE loss 0.100075 (0.112891)\n",
      "16 MSE loss 0.121974 (0.112768)\n",
      "17 MSE loss 0.111438 (0.112964)\n",
      "18 MSE loss 0.094985 (0.112963)\n",
      "19 MSE loss 0.077713 (0.112664)\n",
      "20 MSE loss 0.111036 (0.112721)\n",
      "21 MSE loss 0.077857 (0.112585)\n",
      "22 MSE loss 0.107976 (0.112492)\n",
      "23 MSE loss 0.099104 (0.112269)\n",
      "24 MSE loss 0.073264 (0.112159)\n",
      "25 MSE loss 0.082852 (0.111898)\n",
      "26 MSE loss 0.078731 (0.111685)\n",
      "27 MSE loss 0.095744 (0.111725)\n",
      "28 MSE loss 0.074620 (0.111599)\n",
      "29 MSE loss 0.121482 (0.111628)\n",
      "30 MSE loss 0.115720 (0.111638)\n",
      "31 MSE loss 0.090344 (0.111668)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e590ffaea486f91739bde03f4961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.103758 (0.120509)\n",
      "1 MSE loss 0.122528 (0.115948)\n",
      "2 MSE loss 0.093741 (0.115264)\n",
      "3 MSE loss 0.127610 (0.113888)\n",
      "4 MSE loss 0.108552 (0.113203)\n",
      "5 MSE loss 0.099114 (0.112985)\n",
      "6 MSE loss 0.090917 (0.113060)\n",
      "7 MSE loss 0.087648 (0.112918)\n",
      "8 MSE loss 0.096545 (0.113057)\n",
      "9 MSE loss 0.118602 (0.113430)\n",
      "10 MSE loss 0.105957 (0.113453)\n",
      "11 MSE loss 0.103780 (0.113497)\n",
      "12 MSE loss 0.111821 (0.113345)\n",
      "13 MSE loss 0.082380 (0.113381)\n",
      "14 MSE loss 0.096025 (0.113309)\n",
      "15 MSE loss 0.107472 (0.113315)\n",
      "16 MSE loss 0.096726 (0.113409)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../checkpoints/resnet_checkpoint.pth\n",
      "Resnet: 1549462\n",
      "Length of train loader: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700406e2e2b942948f5c12f8f4654853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE loss 0.187945 (0.202209)\n",
      "1 MSE loss 0.215151 (0.199542)\n",
      "2 MSE loss 0.161203 (0.197765)\n",
      "3 MSE loss 0.185505 (0.196285)\n",
      "4 MSE loss 0.212481 (0.194219)\n",
      "5 MSE loss 0.196179 (0.192481)\n",
      "6 MSE loss 0.199611 (0.190678)\n",
      "7 MSE loss 0.200428 (0.189306)\n",
      "8 MSE loss 0.161477 (0.187820)\n",
      "9 MSE loss 0.179395 (0.186679)\n",
      "10 MSE loss 0.168201 (0.186011)\n",
      "11 MSE loss 0.177966 (0.185526)\n",
      "12 MSE loss 0.171856 (0.184945)\n",
      "13 MSE loss 0.198820 (0.184329)\n",
      "14 MSE loss 0.177302 (0.183883)\n",
      "15 MSE loss 0.210858 (0.183049)\n",
      "16 MSE loss 0.187919 (0.182320)\n",
      "17 MSE loss 0.167979 (0.181660)\n",
      "18 MSE loss 0.205833 (0.181122)\n",
      "19 MSE loss 0.158599 (0.180518)\n",
      "20 MSE loss 0.164044 (0.180051)\n",
      "21 MSE loss 0.142907 (0.179390)\n",
      "22 MSE loss 0.165907 (0.178741)\n",
      "23 MSE loss 0.197914 (0.178121)\n",
      "24 MSE loss 0.144587 (0.177528)\n",
      "25 MSE loss 0.182559 (0.176985)\n",
      "26 MSE loss 0.161905 (0.176521)\n",
      "27 MSE loss 0.178814 (0.176163)\n",
      "28 MSE loss 0.148479 (0.175686)\n",
      "29 MSE loss 0.164143 (0.175157)\n",
      "30 MSE loss 0.147473 (0.174621)\n",
      "31 MSE loss 0.157017 (0.174142)\n",
      "32 MSE loss 0.139121 (0.173683)\n",
      "33 MSE loss 0.190033 (0.173296)\n",
      "34 MSE loss 0.174776 (0.172892)\n",
      "35 MSE loss 0.173452 (0.172363)\n",
      "36 MSE loss 0.178027 (0.172002)\n",
      "37 MSE loss 0.115628 (0.171642)\n",
      "38 MSE loss 0.147660 (0.171165)\n",
      "39 MSE loss 0.141826 (0.170814)\n",
      "40 MSE loss 0.168228 (0.170589)\n",
      "41 MSE loss 0.152743 (0.170173)\n",
      "42 MSE loss 0.141033 (0.169730)\n",
      "43 MSE loss 0.138496 (0.169387)\n",
      "44 MSE loss 0.140046 (0.169123)\n",
      "45 MSE loss 0.144549 (0.168814)\n",
      "46 MSE loss 0.166045 (0.168430)\n",
      "47 MSE loss 0.142451 (0.168121)\n",
      "48 MSE loss 0.150469 (0.167859)\n",
      "49 MSE loss 0.135968 (0.167439)\n",
      "50 MSE loss 0.148668 (0.167139)\n",
      "51 MSE loss 0.145980 (0.166732)\n",
      "52 MSE loss 0.163208 (0.166382)\n",
      "53 MSE loss 0.141030 (0.166103)\n",
      "54 MSE loss 0.134499 (0.165885)\n",
      "55 MSE loss 0.151080 (0.165547)\n",
      "56 MSE loss 0.162802 (0.165309)\n",
      "57 MSE loss 0.162666 (0.165107)\n",
      "58 MSE loss 0.131164 (0.164804)\n",
      "59 MSE loss 0.129249 (0.164619)\n",
      "60 MSE loss 0.124617 (0.164414)\n",
      "61 MSE loss 0.157699 (0.164178)\n",
      "62 MSE loss 0.156216 (0.163888)\n",
      "63 MSE loss 0.134825 (0.163592)\n",
      "64 MSE loss 0.146257 (0.163347)\n",
      "65 MSE loss 0.125822 (0.163079)\n",
      "66 MSE loss 0.152127 (0.162793)\n",
      "67 MSE loss 0.115667 (0.162495)\n",
      "68 MSE loss 0.173158 (0.162303)\n",
      "69 MSE loss 0.146369 (0.162068)\n",
      "70 MSE loss 0.142246 (0.161844)\n",
      "71 MSE loss 0.122859 (0.161579)\n",
      "72 MSE loss 0.117321 (0.161304)\n",
      "73 MSE loss 0.155079 (0.161024)\n",
      "74 MSE loss 0.125503 (0.160739)\n",
      "75 MSE loss 0.150351 (0.160448)\n",
      "76 MSE loss 0.136685 (0.160216)\n",
      "77 MSE loss 0.174009 (0.160058)\n",
      "78 MSE loss 0.155683 (0.159832)\n",
      "79 MSE loss 0.142534 (0.159621)\n",
      "80 MSE loss 0.145840 (0.159367)\n",
      "81 MSE loss 0.163713 (0.159182)\n",
      "82 MSE loss 0.116937 (0.158986)\n",
      "83 MSE loss 0.134897 (0.158783)\n",
      "84 MSE loss 0.143407 (0.158531)\n",
      "85 MSE loss 0.170843 (0.158335)\n",
      "86 MSE loss 0.111193 (0.158089)\n",
      "87 MSE loss 0.157642 (0.157847)\n",
      "88 MSE loss 0.144650 (0.157658)\n",
      "89 MSE loss 0.154880 (0.157469)\n",
      "90 MSE loss 0.139290 (0.157253)\n",
      "91 MSE loss 0.153649 (0.157075)\n",
      "92 MSE loss 0.122776 (0.156902)\n",
      "93 MSE loss 0.101718 (0.156701)\n",
      "94 MSE loss 0.101109 (0.156532)\n",
      "95 MSE loss 0.130553 (0.156348)\n",
      "96 MSE loss 0.118615 (0.156138)\n",
      "97 MSE loss 0.116135 (0.155964)\n",
      "98 MSE loss 0.119600 (0.155743)\n",
      "99 MSE loss 0.100977 (0.155540)\n",
      "100 MSE loss 0.161588 (0.155342)\n",
      "101 MSE loss 0.128590 (0.155111)\n",
      "102 MSE loss 0.144746 (0.154914)\n",
      "103 MSE loss 0.098118 (0.154732)\n",
      "104 MSE loss 0.132218 (0.154550)\n",
      "105 MSE loss 0.118806 (0.154369)\n",
      "106 MSE loss 0.139151 (0.154141)\n",
      "107 MSE loss 0.127232 (0.153949)\n",
      "108 MSE loss 0.109138 (0.153800)\n",
      "109 MSE loss 0.118679 (0.153616)\n",
      "110 MSE loss 0.120282 (0.153405)\n",
      "111 MSE loss 0.128962 (0.153219)\n",
      "112 MSE loss 0.113668 (0.153048)\n",
      "113 MSE loss 0.183689 (0.152937)\n",
      "114 MSE loss 0.136815 (0.152789)\n",
      "115 MSE loss 0.123494 (0.152610)\n",
      "116 MSE loss 0.111761 (0.152448)\n",
      "117 MSE loss 0.110311 (0.152279)\n",
      "118 MSE loss 0.126905 (0.152100)\n",
      "119 MSE loss 0.145203 (0.151910)\n",
      "120 MSE loss 0.126730 (0.151774)\n",
      "121 MSE loss 0.134303 (0.151611)\n",
      "122 MSE loss 0.137604 (0.151495)\n",
      "123 MSE loss 0.122137 (0.151329)\n",
      "124 MSE loss 0.120897 (0.151140)\n",
      "125 MSE loss 0.123203 (0.150988)\n",
      "126 MSE loss 0.104421 (0.150815)\n",
      "127 MSE loss 0.108681 (0.150632)\n",
      "128 MSE loss 0.136021 (0.150471)\n",
      "129 MSE loss 0.134234 (0.150301)\n",
      "130 MSE loss 0.104806 (0.150128)\n",
      "131 MSE loss 0.095802 (0.149936)\n",
      "132 MSE loss 0.114266 (0.149775)\n",
      "133 MSE loss 0.170234 (0.149617)\n",
      "134 MSE loss 0.122518 (0.149443)\n",
      "135 MSE loss 0.127887 (0.149306)\n",
      "136 MSE loss 0.108014 (0.149129)\n",
      "137 MSE loss 0.157279 (0.148987)\n",
      "138 MSE loss 0.138681 (0.148852)\n",
      "139 MSE loss 0.125748 (0.148717)\n",
      "140 MSE loss 0.125440 (0.148577)\n",
      "141 MSE loss 0.137799 (0.148435)\n",
      "142 MSE loss 0.113656 (0.148312)\n",
      "143 MSE loss 0.145252 (0.148155)\n",
      "144 MSE loss 0.118749 (0.148000)\n",
      "145 MSE loss 0.116624 (0.147848)\n",
      "146 MSE loss 0.111623 (0.147718)\n",
      "147 MSE loss 0.124413 (0.147571)\n",
      "148 MSE loss 0.115795 (0.147427)\n",
      "149 MSE loss 0.131295 (0.147294)\n",
      "150 MSE loss 0.116932 (0.147158)\n",
      "151 MSE loss 0.098463 (0.147019)\n",
      "152 MSE loss 0.120684 (0.146894)\n",
      "153 MSE loss 0.120347 (0.146737)\n",
      "154 MSE loss 0.128869 (0.146603)\n",
      "155 MSE loss 0.144139 (0.146473)\n",
      "156 MSE loss 0.153139 (0.146350)\n",
      "157 MSE loss 0.094918 (0.146202)\n",
      "158 MSE loss 0.112954 (0.146073)\n",
      "159 MSE loss 0.127481 (0.145940)\n",
      "160 MSE loss 0.113690 (0.145774)\n",
      "161 MSE loss 0.124696 (0.145620)\n",
      "162 MSE loss 0.123692 (0.145486)\n",
      "163 MSE loss 0.125630 (0.145393)\n",
      "164 MSE loss 0.115545 (0.145255)\n",
      "165 MSE loss 0.095891 (0.145104)\n",
      "166 MSE loss 0.131036 (0.144986)\n",
      "167 MSE loss 0.105639 (0.144831)\n",
      "168 MSE loss 0.131691 (0.144675)\n",
      "169 MSE loss 0.101778 (0.144552)\n",
      "170 MSE loss 0.122920 (0.144415)\n",
      "171 MSE loss 0.120047 (0.144280)\n",
      "172 MSE loss 0.138023 (0.144140)\n",
      "173 MSE loss 0.117263 (0.144004)\n",
      "174 MSE loss 0.134673 (0.143891)\n",
      "175 MSE loss 0.100218 (0.143755)\n",
      "176 MSE loss 0.121632 (0.143641)\n",
      "177 MSE loss 0.092980 (0.143522)\n",
      "178 MSE loss 0.121560 (0.143393)\n",
      "179 MSE loss 0.116251 (0.143251)\n",
      "180 MSE loss 0.137008 (0.143172)\n",
      "181 MSE loss 0.107439 (0.143065)\n",
      "182 MSE loss 0.122375 (0.142947)\n",
      "183 MSE loss 0.127118 (0.142839)\n",
      "184 MSE loss 0.106819 (0.142731)\n",
      "185 MSE loss 0.101274 (0.142602)\n",
      "186 MSE loss 0.119907 (0.142486)\n",
      "187 MSE loss 0.075702 (0.142353)\n",
      "188 MSE loss 0.156146 (0.142221)\n",
      "189 MSE loss 0.145291 (0.142122)\n",
      "190 MSE loss 0.097373 (0.142003)\n",
      "191 MSE loss 0.133059 (0.141890)\n",
      "192 MSE loss 0.113584 (0.141757)\n",
      "193 MSE loss 0.144090 (0.141624)\n",
      "194 MSE loss 0.115185 (0.141504)\n",
      "195 MSE loss 0.096259 (0.141396)\n",
      "196 MSE loss 0.094078 (0.141282)\n",
      "197 MSE loss 0.136143 (0.141154)\n",
      "198 MSE loss 0.108447 (0.141014)\n",
      "199 MSE loss 0.129871 (0.140917)\n",
      "200 MSE loss 0.096861 (0.140803)\n",
      "201 MSE loss 0.123391 (0.140681)\n",
      "202 MSE loss 0.122784 (0.140558)\n",
      "203 MSE loss 0.105863 (0.140454)\n",
      "204 MSE loss 0.092919 (0.140344)\n",
      "205 MSE loss 0.113070 (0.140236)\n",
      "206 MSE loss 0.121527 (0.140111)\n",
      "207 MSE loss 0.130445 (0.140010)\n",
      "208 MSE loss 0.123901 (0.139917)\n"
     ]
    }
   ],
   "source": [
    "train(root_path) #218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
